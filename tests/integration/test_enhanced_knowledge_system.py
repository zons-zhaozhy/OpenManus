#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆçŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯çŸ¥è¯†åº“ç®¡ç†ã€æ–‡æ¡£å¤„ç†ã€å‘é‡å­˜å‚¨ç­‰åŠŸèƒ½
"""

import asyncio
import json
import tempfile
from pathlib import Path

from loguru import logger

from app.modules.knowledge_base.core.knowledge_manager import KnowledgeCategory
from app.modules.knowledge_base.enhanced_service import EnhancedKnowledgeService
from app.modules.knowledge_base.types import KnowledgeQuery


async def test_knowledge_base_creation():
    """æµ‹è¯•çŸ¥è¯†åº“åˆ›å»ºåŠŸèƒ½"""
    logger.info("ğŸ§ª æµ‹è¯•çŸ¥è¯†åº“åˆ›å»ºåŠŸèƒ½")

    service = EnhancedKnowledgeService()

    # åˆ›å»ºæµ‹è¯•çŸ¥è¯†åº“
    test_cases = [
        {
            "name": "éœ€æ±‚åˆ†ææŒ‡å—",
            "description": "è½¯ä»¶éœ€æ±‚åˆ†æçš„æ–¹æ³•è®ºå’Œæœ€ä½³å®è·µ",
            "category": KnowledgeCategory.REQUIREMENTS_ANALYSIS,
            "tags": ["éœ€æ±‚åˆ†æ", "è½¯ä»¶å·¥ç¨‹", "æœ€ä½³å®è·µ"],
        },
        {
            "name": "ç³»ç»Ÿæ¶æ„è®¾è®¡",
            "description": "ç³»ç»Ÿæ¶æ„è®¾è®¡åŸåˆ™å’Œæ¨¡å¼",
            "category": KnowledgeCategory.SYSTEM_DESIGN,
            "tags": ["æ¶æ„è®¾è®¡", "è®¾è®¡æ¨¡å¼"],
        },
        {
            "name": "ç¼–ç è§„èŒƒæ‰‹å†Œ",
            "description": "ä»£ç è´¨é‡å’Œç¼–ç¨‹è§„èŒƒ",
            "category": KnowledgeCategory.CODING_STANDARDS,
            "tags": ["ç¼–ç è§„èŒƒ", "ä»£ç è´¨é‡"],
        },
    ]

    created_kbs = []
    for case in test_cases:
        kb = service.create_knowledge_base(**case)
        if kb:
            created_kbs.append(kb)
            logger.success(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ: {kb.name} (ID: {kb.id})")
        else:
            logger.error(f"âŒ çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥: {case['name']}")

    return created_kbs


async def test_document_upload():
    """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ åŠŸèƒ½"""
    logger.info("ğŸ“„ æµ‹è¯•æ–‡æ¡£ä¸Šä¼ åŠŸèƒ½")

    service = EnhancedKnowledgeService()

    # è·å–å·²æœ‰çš„çŸ¥è¯†åº“
    knowledge_bases = service.list_knowledge_bases()
    if not knowledge_bases:
        logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„çŸ¥è¯†åº“")
        return []

    kb_id = knowledge_bases[0]["id"]
    logger.info(f"ä½¿ç”¨çŸ¥è¯†åº“: {knowledge_bases[0]['name']} (ID: {kb_id})")

    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_documents = [
        {
            "filename": "requirements_analysis_guide.md",
            "content": """# éœ€æ±‚åˆ†ææŒ‡å—

## 1. éœ€æ±‚æ”¶é›†é˜¶æ®µ

### 1.1 å¹²ç³»äººè¯†åˆ«
- è¯†åˆ«æ‰€æœ‰åˆ©ç›Šç›¸å…³è€…
- æ˜ç¡®å„å¹²ç³»äººçš„èŒè´£å’Œæƒé™
- å»ºç«‹æ²Ÿé€šæ¸ é“

### 1.2 éœ€æ±‚æ”¶é›†æŠ€æœ¯
- é¢è°ˆæ³•ï¼šæ·±å…¥äº†è§£ç”¨æˆ·éœ€æ±‚
- é—®å·è°ƒæŸ¥ï¼šå¹¿æ³›æ”¶é›†æ„è§
- åŸå‹æ³•ï¼šå¿«é€ŸéªŒè¯éœ€æ±‚ç†è§£
- å¤´è„‘é£æš´ï¼šæ¿€å‘åˆ›æ–°æƒ³æ³•

## 2. éœ€æ±‚åˆ†æé˜¶æ®µ

### 2.1 åŠŸèƒ½éœ€æ±‚åˆ†æ
- æ˜ç¡®ç³»ç»Ÿå¿…é¡»æä¾›çš„åŠŸèƒ½
- å®šä¹‰è¾“å…¥è¾“å‡ºè§„æ ¼
- ç¡®å®šå¤„ç†é€»è¾‘

### 2.2 éåŠŸèƒ½éœ€æ±‚åˆ†æ
- æ€§èƒ½éœ€æ±‚ï¼šå“åº”æ—¶é—´ã€ååé‡
- å®‰å…¨éœ€æ±‚ï¼šè®¿é—®æ§åˆ¶ã€æ•°æ®ä¿æŠ¤
- å¯ç”¨æ€§éœ€æ±‚ï¼šç•Œé¢å‹å¥½æ€§ã€æ˜“ç”¨æ€§
- å¯é æ€§éœ€æ±‚ï¼šæ•…éšœå¤„ç†ã€æ¢å¤èƒ½åŠ›

## 3. éœ€æ±‚éªŒè¯

### 3.1 éœ€æ±‚è¯„å®¡
- å®Œæ•´æ€§æ£€æŸ¥
- ä¸€è‡´æ€§éªŒè¯
- å¯è¡Œæ€§åˆ†æ
- å¯æµ‹è¯•æ€§è¯„ä¼°

### 3.2 åŸå‹éªŒè¯
- ç•Œé¢åŸå‹
- åŠŸèƒ½åŸå‹
- ç”¨æˆ·ä½“éªŒæµ‹è¯•
""",
        },
        {
            "filename": "system_design_principles.txt",
            "content": """ç³»ç»Ÿè®¾è®¡åŸºæœ¬åŸåˆ™

1. å•ä¸€èŒè´£åŸåˆ™ (SRP)
æ¯ä¸ªç±»åº”è¯¥åªæœ‰ä¸€ä¸ªå¼•èµ·å®ƒå˜åŒ–çš„åŸå› ã€‚

2. å¼€é—­åŸåˆ™ (OCP)
è½¯ä»¶å®ä½“åº”è¯¥å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­ã€‚

3. é‡Œæ°æ›¿æ¢åŸåˆ™ (LSP)
å­ç±»å¯¹è±¡åº”è¯¥èƒ½å¤Ÿæ›¿æ¢å…¶çˆ¶ç±»å¯¹è±¡ã€‚

4. æ¥å£éš”ç¦»åŸåˆ™ (ISP)
ä¸åº”è¯¥å¼ºè¿«å®¢æˆ·ä¾èµ–å®ƒä»¬ä¸ä½¿ç”¨çš„æ¥å£ã€‚

5. ä¾èµ–å€’ç½®åŸåˆ™ (DIP)
é«˜å±‚æ¨¡å—ä¸åº”è¯¥ä¾èµ–ä½å±‚æ¨¡å—ï¼Œä¸¤è€…éƒ½åº”è¯¥ä¾èµ–æŠ½è±¡ã€‚

æ¶æ„æ¨¡å¼ï¼š

1. åˆ†å±‚æ¶æ„
- è¡¨ç¤ºå±‚ï¼šç”¨æˆ·ç•Œé¢
- ä¸šåŠ¡å±‚ï¼šä¸šåŠ¡é€»è¾‘
- æ•°æ®å±‚ï¼šæ•°æ®è®¿é—®

2. å¾®æœåŠ¡æ¶æ„
- æœåŠ¡æ‹†åˆ†
- ç‹¬ç«‹éƒ¨ç½²
- æ¾è€¦åˆ

3. äº‹ä»¶é©±åŠ¨æ¶æ„
- å¼‚æ­¥é€šä¿¡
- äº‹ä»¶å‘å¸ƒè®¢é˜…
- è§£è€¦åˆ
""",
        },
    ]

    uploaded_docs = []

    for doc_info in test_documents:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=f"_{doc_info['filename']}", delete=False, encoding="utf-8"
        ) as temp_file:
            temp_file.write(doc_info["content"])
            temp_file_path = temp_file.name

        try:
            # ä¸Šä¼ æ–‡æ¡£
            result = await service.upload_document(
                kb_id=kb_id,
                file_path=temp_file_path,
                title=doc_info["filename"].split(".")[0],
            )

            if result:
                uploaded_docs.append(result)
                logger.success(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {result['title']}")
                logger.info(f"   - æ–‡æ¡£ID: {result['document_id']}")
                logger.info(f"   - çŸ¥è¯†ç‚¹æ•°é‡: {result['knowledge_points_count']}")
                logger.info(f"   - å…³é”®è¯æ•°é‡: {result['keywords_count']}")
                logger.info(f"   - æ‘˜è¦: {result['summary'][:100]}...")
            else:
                logger.error(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {doc_info['filename']}")

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                Path(temp_file_path).unlink()
            except OSError:
                pass

    return uploaded_docs


async def test_knowledge_search():
    """æµ‹è¯•çŸ¥è¯†æœç´¢åŠŸèƒ½"""
    logger.info("ğŸ” æµ‹è¯•çŸ¥è¯†æœç´¢åŠŸèƒ½")

    service = EnhancedKnowledgeService()

    # æµ‹è¯•æœç´¢æŸ¥è¯¢
    test_queries = [
        "éœ€æ±‚åˆ†æ",
        "ç³»ç»Ÿè®¾è®¡åŸåˆ™",
        "å¾®æœåŠ¡æ¶æ„",
        "ç”¨æˆ·ç•Œé¢è®¾è®¡",
        "æ•°æ®åº“è®¾è®¡",
    ]

    search_results = []

    for query_text in test_queries:
        logger.info(f"æœç´¢: {query_text}")

        query = KnowledgeQuery(query_text=query_text, limit=5, min_confidence=0.0)

        result = await service.search_knowledge(query)

        logger.info(f"   æ‰¾åˆ° {result.total_results} ä¸ªç»“æœ")
        for i, item in enumerate(result.results[:3]):  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            logger.info(f"   {i+1}. åˆ†æ•°: {item.get('score', 0):.2f}")
            logger.info(f"      å†…å®¹: {item.get('content', '')[:100]}...")

        search_results.append(
            {
                "query": query_text,
                "total_results": result.total_results,
                "results": result.results,
            }
        )

    return search_results


async def test_system_stats():
    """æµ‹è¯•ç³»ç»Ÿç»Ÿè®¡åŠŸèƒ½"""
    logger.info("ğŸ“Š æµ‹è¯•ç³»ç»Ÿç»Ÿè®¡åŠŸèƒ½")

    service = EnhancedKnowledgeService()

    # è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    stats = service.get_system_stats()

    logger.info("ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"  çŸ¥è¯†åº“æ€»æ•°: {stats['knowledge_bases']['total']}")
    logger.info(f"  æ´»è·ƒçŸ¥è¯†åº“: {stats['knowledge_bases']['active']}")
    logger.info(f"  æ–‡æ¡£æ€»æ•°: {stats['documents']['total']}")
    logger.info(f"  å­˜å‚¨å¤§å°: {stats['storage']['total_size_mb']} MB")
    logger.info(f"  å‘é‡å­˜å‚¨å¯ç”¨: {stats['vector_store']['available']}")

    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    if stats["knowledge_bases"]["by_category"]:
        logger.info("  æŒ‰åˆ†ç±»ç»Ÿè®¡:")
        for category, count in stats["knowledge_bases"]["by_category"].items():
            logger.info(f"    {category}: {count}")

    return stats


async def test_knowledge_gaps_analysis():
    """æµ‹è¯•çŸ¥è¯†åº“ç¼ºå£åˆ†æ"""
    logger.info("ğŸ” æµ‹è¯•çŸ¥è¯†åº“ç¼ºå£åˆ†æ")

    service = EnhancedKnowledgeService()

    # è·å–æ‰€æœ‰çŸ¥è¯†åº“ID
    knowledge_bases = service.list_knowledge_bases()
    kb_ids = [kb["id"] for kb in knowledge_bases]

    if not kb_ids:
        logger.warning("âš ï¸ æ²¡æœ‰çŸ¥è¯†åº“å¯ä¾›åˆ†æ")
        return {}

    # æ‰§è¡Œç¼ºå£åˆ†æ
    analysis = service.analyze_knowledge_gaps(kb_ids)

    logger.info("çŸ¥è¯†åº“ç¼ºå£åˆ†æç»“æœ:")
    logger.info(f"  åˆ†æçš„çŸ¥è¯†åº“æ•°é‡: {analysis['total_knowledge_bases']}")

    # è¦†ç›–èŒƒå›´åˆ†æ
    if analysis["coverage_analysis"]:
        logger.info("  è¦†ç›–çš„é¢†åŸŸ:")
        for category, kb_list in analysis["coverage_analysis"].items():
            logger.info(f"    {category}: {len(kb_list)} ä¸ªçŸ¥è¯†åº“")

    # ç¼ºå£é¢†åŸŸ
    if analysis["gap_areas"]:
        logger.info("  ç¼ºå¤±çš„é¢†åŸŸ:")
        for area in analysis["gap_areas"]:
            logger.info(f"    - {area}")

    # æ”¹è¿›å»ºè®®
    if analysis["recommendations"]:
        logger.info("  æ”¹è¿›å»ºè®®:")
        for recommendation in analysis["recommendations"]:
            logger.info(f"    - {recommendation}")

    return analysis


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆçŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•")

    test_results = {}

    try:
        # 1. æµ‹è¯•çŸ¥è¯†åº“åˆ›å»º
        created_kbs = await test_knowledge_base_creation()
        test_results["knowledge_base_creation"] = {
            "success": len(created_kbs) > 0,
            "created_count": len(created_kbs),
        }

        # 2. æµ‹è¯•æ–‡æ¡£ä¸Šä¼ 
        uploaded_docs = await test_document_upload()
        test_results["document_upload"] = {
            "success": len(uploaded_docs) > 0,
            "uploaded_count": len(uploaded_docs),
        }

        # 3. æµ‹è¯•çŸ¥è¯†æœç´¢
        search_results = await test_knowledge_search()
        test_results["knowledge_search"] = {
            "success": len(search_results) > 0,
            "queries_tested": len(search_results),
        }

        # 4. æµ‹è¯•ç³»ç»Ÿç»Ÿè®¡
        stats = await test_system_stats()
        test_results["system_stats"] = {"success": bool(stats), "stats": stats}

        # 5. æµ‹è¯•ç¼ºå£åˆ†æ
        gaps_analysis = await test_knowledge_gaps_analysis()
        test_results["gaps_analysis"] = {
            "success": bool(gaps_analysis),
            "analysis": gaps_analysis,
        }

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        test_results["error"] = str(e)

    # è¾“å‡ºæµ‹è¯•æ‘˜è¦
    logger.info("\nğŸ“‹ æµ‹è¯•æ‘˜è¦:")
    for test_name, result in test_results.items():
        if test_name == "error":
            logger.error(f"  âŒ æµ‹è¯•å¼‚å¸¸: {result}")
        else:
            status = "âœ… æˆåŠŸ" if result.get("success") else "âŒ å¤±è´¥"
            logger.info(f"  {status} {test_name}")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(
        "enhanced_knowledge_system_test_results.json", "w", encoding="utf-8"
    ) as f:
        json.dump(test_results, f, ensure_ascii=False, indent=2, default=str)

    logger.info(
        f"\nğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: enhanced_knowledge_system_test_results.json"
    )

    return test_results


if __name__ == "__main__":
    asyncio.run(main())
