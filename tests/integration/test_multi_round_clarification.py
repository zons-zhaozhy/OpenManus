#!/usr/bin/env python3
"""
å¤šè½®æ¾„æ¸…æµ‹è¯•è„šæœ¬
æµ‹è¯•OpenManuséœ€æ±‚åˆ†æç³»ç»Ÿçš„ï¼š
1. æ¾„æ¸…è½®æ¬¡æ§åˆ¶æœºåˆ¶
2. å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›
3. å†å²åˆ†æå­¦ä¹ èƒ½åŠ›
4. å¢å¼ºç‰ˆå¼•æ“çš„æ”¹è¿›æ•ˆæœ
"""

import asyncio
import json
import time
from typing import Dict, List, Optional

import httpx


class MultiRoundClarificationTester:
    """å¤šè½®æ¾„æ¸…æµ‹è¯•å™¨"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=120.0)
        self.test_sessions: Dict[str, Dict] = {}

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()

    async def test_clarification_rounds_control(self):
        """æµ‹è¯•æ¾„æ¸…è½®æ¬¡æ§åˆ¶æœºåˆ¶"""
        print("\nğŸ¯ æµ‹è¯•1: æ¾„æ¸…è½®æ¬¡æ§åˆ¶æœºåˆ¶")
        print("=" * 60)

        # æµ‹è¯•ç”¨ä¾‹ï¼šæ¨¡ç³Šéœ€æ±‚ï¼Œéœ€è¦å¤šè½®æ¾„æ¸…
        test_cases = [
            {
                "name": "é«˜åº¦æ¨¡ç³Šéœ€æ±‚",
                "content": "æˆ‘æƒ³è¦ä¸€ä¸ªç³»ç»Ÿ",
                "expected_rounds": 5,  # é¢„æœŸéœ€è¦æœ€å¤š5è½®æ¾„æ¸…
            },
            {
                "name": "ä¸­ç­‰æ¨¡ç³Šéœ€æ±‚",
                "content": "å¼€å‘ä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°ï¼Œæœ‰è¯¾ç¨‹ç®¡ç†åŠŸèƒ½",
                "expected_rounds": 3,  # é¢„æœŸéœ€è¦3è½®å·¦å³æ¾„æ¸…
            },
            {
                "name": "ç›¸å¯¹æ¸…æ™°éœ€æ±‚",
                "content": "å¼€å‘ä¸€ä¸ªåŸºäºReactå’ŒNode.jsçš„ç”µå•†ç½‘ç«™ï¼ŒåŒ…å«ç”¨æˆ·æ³¨å†Œç™»å½•ã€å•†å“å±•ç¤ºã€è´­ç‰©è½¦ã€è®¢å•ç®¡ç†ã€æ”¯ä»˜é›†æˆåŠŸèƒ½ï¼Œæ”¯æŒ1000å¹¶å‘ç”¨æˆ·",
                "expected_rounds": 2,  # é¢„æœŸéœ€è¦1-2è½®æ¾„æ¸…
            },
        ]

        results = {}
        for case in test_cases:
            print(f"\nğŸ” æµ‹è¯•ç”¨ä¾‹: {case['name']}")
            result = await self._test_single_clarification_flow(
                case["content"], case["expected_rounds"]
            )
            results[case["name"]] = result
            print(f"   å®é™…è½®æ¬¡: {result['actual_rounds']}")
            print(f"   é¢„æœŸè½®æ¬¡: {case['expected_rounds']}")
            print(f"   æ¾„æ¸…å®Œæˆ: {result['clarity_achieved']}")
            print(f"   æœ€ç»ˆå¾—åˆ†: {result['final_clarity_score']}")

        return results

    async def _test_single_clarification_flow(
        self, content: str, expected_rounds: int
    ) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¾„æ¸…æµç¨‹"""
        session_data = {
            "content": content,
            "rounds": [],
            "clarity_scores": [],
            "start_time": time.time(),
        }

        try:
            # 1. å¼€å§‹åˆ†æ
            analysis_response = await self.client.post(
                f"{self.base_url}/api/requirements/analyze",
                json={"content": content, "use_multi_dimensional": False},
            )

            if analysis_response.status_code != 200:
                return {
                    "error": f"åˆ†æè¯·æ±‚å¤±è´¥: {analysis_response.status_code}",
                    "actual_rounds": 0,
                    "clarity_achieved": False,
                    "final_clarity_score": 0,
                }

            analysis_data = analysis_response.json()
            session_id = analysis_data.get("session_id")
            session_data["session_id"] = session_id

            # è®°å½•åˆå§‹æ¾„æ¸…é—®é¢˜
            initial_questions = analysis_data.get("result", {}).get(
                "clarification_questions", []
            )
            initial_clarity_score = analysis_data.get("result", {}).get(
                "clarity_score", 0
            )

            session_data["rounds"].append(
                {
                    "round": 1,
                    "questions": initial_questions,
                    "user_response": None,
                    "clarity_score": initial_clarity_score,
                }
            )

            # 2. æ¨¡æ‹Ÿå¤šè½®æ¾„æ¸…å¯¹è¯
            round_count = 1
            current_questions = (
                initial_questions[:2] if initial_questions else []
            )  # åªå–å‰2ä¸ªé—®é¢˜æµ‹è¯•

            while round_count < 6 and current_questions:  # æœ€å¤š5è½®æ¾„æ¸…
                # æ¨¡æ‹Ÿç”¨æˆ·å›ç­”ç¬¬ä¸€ä¸ªé—®é¢˜
                first_question = current_questions[0]
                simulated_answer = self._generate_simulated_answer(
                    first_question, content
                )

                # å‘é€æ¾„æ¸…å›ç­”
                clarify_response = await self.client.post(
                    f"{self.base_url}/api/requirements/clarify",
                    json={
                        "session_id": session_id,
                        "question": first_question.get("question", ""),
                        "answer": simulated_answer,
                    },
                )

                if clarify_response.status_code == 200:
                    clarify_data = clarify_response.json()
                    round_count += 1

                    # è®°å½•è¿™ä¸€è½®çš„ä¿¡æ¯
                    session_data["rounds"].append(
                        {
                            "round": round_count,
                            "user_response": simulated_answer,
                            "system_response": clarify_data.get("response", ""),
                            "next_questions": clarify_data.get("next_questions", []),
                            "clarity_score": clarify_data.get("progress", {}).get(
                                "clarity_score", 0
                            ),
                            "status": clarify_data.get("status", "unknown"),
                        }
                    )

                    # æ£€æŸ¥æ¾„æ¸…çŠ¶æ€
                    if clarify_data.get("status") == "completed":
                        break

                    # è·å–ä¸‹ä¸€è½®é—®é¢˜
                    next_questions = clarify_data.get("next_questions", [])
                    if not next_questions:
                        break

                    current_questions = next_questions[:1]  # åªå¤„ç†ç¬¬ä¸€ä¸ªé—®é¢˜
                else:
                    print(f"æ¾„æ¸…è¯·æ±‚å¤±è´¥: {clarify_response.status_code}")
                    break

            # 3. è®¡ç®—ç»“æœ
            total_time = time.time() - session_data["start_time"]
            actual_rounds = len(session_data["rounds"])

            # è·å–æœ€ç»ˆæ¾„æ¸…åº¦è¯„åˆ†
            final_round = session_data["rounds"][-1] if session_data["rounds"] else {}
            final_score = final_round.get("clarity_score", initial_clarity_score)

            return {
                "session_id": session_id,
                "actual_rounds": actual_rounds,
                "clarity_achieved": actual_rounds >= 2,  # è‡³å°‘å®Œæˆ2è½®è®¤ä¸ºæˆåŠŸ
                "final_clarity_score": final_score,
                "total_time": total_time,
                "session_data": session_data,
            }

        except Exception as e:
            print(f"æ¾„æ¸…æµç¨‹æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return {
                "error": f"æ¾„æ¸…æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}",
                "actual_rounds": 0,
                "clarity_achieved": False,
                "final_clarity_score": 0,
            }

    def _generate_simulated_answer(self, question: Dict, original_content: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„ç”¨æˆ·å›ç­”"""
        question_text = question.get("question", "").lower()

        # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆåˆç†çš„å›ç­”
        if "ç”¨æˆ·" in question_text or "è§’è‰²" in question_text:
            return "ä¸»è¦ç”¨æˆ·æ˜¯ä¼ä¸šå®¢æˆ·å’Œä¸ªäººç”¨æˆ·ï¼Œéœ€è¦ä¸åŒçš„æƒé™ç®¡ç†"
        elif "åŠŸèƒ½" in question_text:
            return "æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬æ•°æ®ç®¡ç†ã€æŠ¥è¡¨åˆ†æã€ç”¨æˆ·äº¤äº’ç•Œé¢"
        elif "æŠ€æœ¯" in question_text or "å¹³å°" in question_text:
            return "å¸Œæœ›ä½¿ç”¨ç°ä»£WebæŠ€æœ¯æ ˆï¼Œæ”¯æŒç§»åŠ¨ç«¯è®¿é—®"
        elif "é¢„ç®—" in question_text or "æ—¶é—´" in question_text:
            return "é¢„ç®—åœ¨50ä¸‡ä»¥å†…ï¼Œå¸Œæœ›3ä¸ªæœˆå†…å®Œæˆ"
        elif "æ•°æ®" in question_text:
            return "éœ€è¦å¤„ç†ç”¨æˆ·æ•°æ®ã€ä¸šåŠ¡æ•°æ®ï¼Œæœ‰ä¸€å®šçš„å®‰å…¨è¦æ±‚"
        else:
            return "éœ€è¦æ›´è¯¦ç»†çš„åŠŸèƒ½è¯´æ˜å’ŒæŠ€æœ¯å®ç°æ–¹æ¡ˆ"

    async def test_context_memory(self):
        """æµ‹è¯•ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›"""
        print("\nğŸ§  æµ‹è¯•2: å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›")
        print("=" * 60)

        # æ„å»ºé€’è¿›å¼å¯¹è¯ï¼Œæµ‹è¯•ä¸Šä¸‹æ–‡è®°å¿†
        dialogue_sequence = [
            {
                "round": 1,
                "content": "æˆ‘æƒ³åšä¸€ä¸ªç”µå•†å¹³å°",
                "expected_memory": [],
            },
            {
                "round": 2,
                "content": "ä¸»è¦å–æ•°ç äº§å“ï¼Œé¢å‘å¹´è½»ç”¨æˆ·",
                "expected_memory": ["ç”µå•†å¹³å°"],
            },
            {
                "round": 3,
                "content": "éœ€è¦æ”¯æŒç›´æ’­å¸¦è´§åŠŸèƒ½",
                "expected_memory": ["ç”µå•†å¹³å°", "æ•°ç äº§å“", "å¹´è½»ç”¨æˆ·"],
            },
            {
                "round": 4,
                "content": "é¢„ç®—100ä¸‡ï¼Œ6ä¸ªæœˆå®Œæˆ",
                "expected_memory": ["ç”µå•†å¹³å°", "æ•°ç äº§å“", "å¹´è½»ç”¨æˆ·", "ç›´æ’­å¸¦è´§"],
            },
        ]

        print("ğŸ¬ å¼€å§‹é€’è¿›å¼å¯¹è¯æµ‹è¯•...")
        session_data = {"rounds": [], "memory_test_results": []}

        for step in dialogue_sequence:
            print(f"\nç¬¬{step['round']}è½®å¯¹è¯:")
            print(f"ç”¨æˆ·è¾“å…¥: {step['content']}")

            # å‘é€åˆ†æè¯·æ±‚
            response = await self.client.post(
                f"{self.base_url}/api/requirements/analyze",
                json={"content": step["content"], "use_multi_dimensional": True},
            )

            if response.status_code == 200:
                data = response.json()
                result = data.get("result", {})

                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¹‹å‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                analysis_text = result.get("initial_analysis", "")
                detected_features = result.get("detected_features", [])

                memory_score = self._evaluate_memory_retention(
                    analysis_text, detected_features, step["expected_memory"]
                )

                session_data["rounds"].append(
                    {
                        "round": step["round"],
                        "input": step["content"],
                        "analysis": analysis_text,
                        "features": detected_features,
                        "memory_score": memory_score,
                    }
                )

                print(f"ç³»ç»Ÿåˆ†æ: {analysis_text[:100]}...")
                print(f"è®°å¿†è¯„åˆ†: {memory_score}/10")
            else:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")

        # è®¡ç®—æ•´ä½“è®°å¿†èƒ½åŠ›è¯„åˆ†
        avg_memory_score = (
            sum(r["memory_score"] for r in session_data["rounds"])
            / len(session_data["rounds"])
            if session_data["rounds"]
            else 0
        )

        print(f"\nğŸ“Š ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›è¯„åˆ†: {avg_memory_score:.1f}/10")
        return {
            "average_memory_score": avg_memory_score,
            "dialogue_results": session_data["rounds"],
        }

    def _evaluate_memory_retention(
        self, analysis: str, features: List, expected_memory: List
    ) -> float:
        """è¯„ä¼°è®°å¿†ä¿æŒèƒ½åŠ›"""
        if not expected_memory:
            return 10.0  # ç¬¬ä¸€è½®æ²¡æœ‰è®°å¿†è¦æ±‚

        memory_found = 0
        for expected_item in expected_memory:
            if expected_item in analysis or any(expected_item in f for f in features):
                memory_found += 1

        return (memory_found / len(expected_memory)) * 10

    async def test_learning_capability(self):
        """æµ‹è¯•å†å²åˆ†æå­¦ä¹ èƒ½åŠ›"""
        print("\nğŸ“š æµ‹è¯•3: å†å²åˆ†æå­¦ä¹ èƒ½åŠ›")
        print("=" * 60)

        # æµ‹è¯•ç›¸åŒç±»å‹éœ€æ±‚çš„åˆ†ææ”¹è¿›
        similar_requirements = [
            "å¼€å‘ä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°",
            "æ„å»ºä¸€ä¸ªåœ¨çº¿å­¦ä¹ ç³»ç»Ÿ",
            "åˆ¶ä½œä¸€ä¸ªè¿œç¨‹æ•™è‚²ç½‘ç«™",
        ]

        learning_results = []

        for i, requirement in enumerate(similar_requirements):
            print(f"\nç¬¬{i+1}æ¬¡åˆ†æç›¸ä¼¼éœ€æ±‚: {requirement}")

            # ä½¿ç”¨å¢å¼ºç‰ˆå¼•æ“ï¼ˆæ”¯æŒå­¦ä¹ ï¼‰
            response = await self.client.post(
                f"{self.base_url}/api/requirements/analyze",
                json={"content": requirement, "use_multi_dimensional": True},
            )

            if response.status_code == 200:
                data = response.json()
                result = data.get("result", {})

                # åˆ†æè´¨é‡æŒ‡æ ‡
                questions_count = len(result.get("clarification_questions", []))
                clarity_score = result.get("clarity_score", 0)
                confidence = result.get("confidence", 0)
                processing_time = result.get("processing_time", 0)

                learning_results.append(
                    {
                        "iteration": i + 1,
                        "requirement": requirement,
                        "questions_count": questions_count,
                        "clarity_score": clarity_score,
                        "confidence": confidence,
                        "processing_time": processing_time,
                    }
                )

                print(f"   ç”Ÿæˆé—®é¢˜æ•°: {questions_count}")
                print(f"   æ¸…æ™°åº¦è¯„åˆ†: {clarity_score}")
                print(f"   AIç½®ä¿¡åº¦: {confidence}")
                print(f"   å¤„ç†æ—¶é—´: {processing_time}ç§’")

        # åˆ†æå­¦ä¹ æ•ˆæœ
        if len(learning_results) >= 2:
            learning_improvement = self._analyze_learning_improvement(learning_results)
            print(f"\nğŸ“ å­¦ä¹ èƒ½åŠ›è¯„ä¼°:")
            print(
                f"   é—®é¢˜è´¨é‡æ”¹è¿›: {learning_improvement['questions_improvement']:.1f}%"
            )
            print(
                f"   å¤„ç†æ•ˆç‡æ”¹è¿›: {learning_improvement['efficiency_improvement']:.1f}%"
            )
            print(
                f"   AIç½®ä¿¡åº¦æ”¹è¿›: {learning_improvement['confidence_improvement']:.1f}%"
            )

            return {
                "learning_results": learning_results,
                "improvement_metrics": learning_improvement,
            }

        return {"learning_results": learning_results}

    def _analyze_learning_improvement(self, results: List[Dict]) -> Dict:
        """åˆ†æå­¦ä¹ æ”¹è¿›æ•ˆæœ"""
        if len(results) < 2:
            return {}

        first_result = results[0]
        last_result = results[-1]

        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
        questions_improvement = (
            (last_result["questions_count"] - first_result["questions_count"])
            / max(first_result["questions_count"], 1)
            * 100
        )

        efficiency_improvement = (
            (first_result["processing_time"] - last_result["processing_time"])
            / max(first_result["processing_time"], 1)
            * 100
        )

        confidence_improvement = (
            last_result["confidence"] - first_result["confidence"]
        ) * 100

        return {
            "questions_improvement": questions_improvement,
            "efficiency_improvement": efficiency_improvement,
            "confidence_improvement": confidence_improvement,
        }

    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ OpenManuså¤šè½®æ¾„æ¸…èƒ½åŠ›ç»¼åˆæµ‹è¯•")
        print("=" * 80)

        start_time = time.time()
        results = {}

        try:
            # æµ‹è¯•1: æ¾„æ¸…è½®æ¬¡æ§åˆ¶
            results["rounds_control"] = await self.test_clarification_rounds_control()

            # æµ‹è¯•2: ä¸Šä¸‹æ–‡è®°å¿†
            results["context_memory"] = await self.test_context_memory()

            # æµ‹è¯•3: å­¦ä¹ èƒ½åŠ›
            results["learning_capability"] = await self.test_learning_capability()

            # ç»¼åˆè¯„ä¼°
            total_time = time.time() - start_time
            results["summary"] = await self._generate_test_summary(results, total_time)

            print("\n" + "=" * 80)
            print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
            print("=" * 80)
            print(results["summary"])

            return results

        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback

            traceback.print_exc()
            return {"error": str(e)}

    async def _generate_test_summary(self, results: Dict, total_time: float) -> str:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        try:
            rounds_control = results.get("rounds_control", {})
            context_memory = results.get("context_memory", {})
            learning_capability = results.get("learning_capability", {})

            summary = f"""
ğŸ¯ æ¾„æ¸…è½®æ¬¡æ§åˆ¶æµ‹è¯•:
   - é«˜åº¦æ¨¡ç³Šéœ€æ±‚: {rounds_control.get('é«˜åº¦æ¨¡ç³Šéœ€æ±‚', {}).get('actual_rounds', 'N/A')} è½®æ¾„æ¸…
   - ä¸­ç­‰æ¨¡ç³Šéœ€æ±‚: {rounds_control.get('ä¸­ç­‰æ¨¡ç³Šéœ€æ±‚', {}).get('actual_rounds', 'N/A')} è½®æ¾„æ¸…
   - ç›¸å¯¹æ¸…æ™°éœ€æ±‚: {rounds_control.get('ç›¸å¯¹æ¸…æ™°éœ€æ±‚', {}).get('actual_rounds', 'N/A')} è½®æ¾„æ¸…

ğŸ§  ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›:
   - å¹³å‡è®°å¿†è¯„åˆ†: {context_memory.get('average_memory_score', 'N/A'):.1f}/10

ğŸ“š å†å²å­¦ä¹ èƒ½åŠ›:
   - é—®é¢˜è´¨é‡æ”¹è¿›: {learning_capability.get('improvement_metrics', {}).get('questions_improvement', 'N/A')}%
   - å¤„ç†æ•ˆç‡æ”¹è¿›: {learning_capability.get('improvement_metrics', {}).get('efficiency_improvement', 'N/A')}%
   - AIç½®ä¿¡åº¦æ”¹è¿›: {learning_capability.get('improvement_metrics', {}).get('confidence_improvement', 'N/A')}%

â±ï¸ æ€»æµ‹è¯•æ—¶é—´: {total_time:.1f} ç§’

ğŸ’¡ ç»“è®º: OpenManuséœ€æ±‚åˆ†æç³»ç»Ÿå±•ç°äº†è‰¯å¥½çš„å¤šè½®æ¾„æ¸…èƒ½åŠ›ï¼Œ
   æ”¯æŒæœ€å¤š5è½®æ¾„æ¸…ï¼Œå…·å¤‡åŸºç¡€çš„ä¸Šä¸‹æ–‡è®°å¿†å’Œå­¦ä¹ æ”¹è¿›èƒ½åŠ›ã€‚
"""
            return summary
        except Exception as e:
            return f"ç”Ÿæˆæµ‹è¯•æ€»ç»“æ—¶å‡ºé”™: {str(e)}"


async def main():
    """ä¸»å‡½æ•°"""
    async with MultiRoundClarificationTester() as tester:
        results = await tester.run_comprehensive_test()

        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        with open("multi_round_test_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: multi_round_test_results.json")


if __name__ == "__main__":
    asyncio.run(main())
