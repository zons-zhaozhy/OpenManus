"""
è´¨é‡å¯¼å‘çš„æ¾„æ¸…å¼•æ“
åŸºäºç”¨æˆ·åé¦ˆé‡æ–°è®¾è®¡ï¼šç›®æ ‡å¯¼å‘ã€é€†å‘æ€ç»´ã€è´¨é‡ä¸ºæœ¬

æ ¸å¿ƒç†å¿µï¼š
1. ç›®æ ‡å¯¼å‘ï¼šæ˜ç¡®æ¯æ¬¡æ¾„æ¸…è¦è¾¾åˆ°çš„å…·ä½“ç›®æ ‡
2. é€†å‘æ€ç»´ï¼šä»æœ€ç»ˆéœ€æ±‚æ–‡æ¡£è´¨é‡å€’æ¨éœ€è¦æ¾„æ¸…çš„å†…å®¹
3. è´¨é‡ä¸ºæœ¬ï¼šä»¥éœ€æ±‚å®Œæ•´æ€§å’Œè´¨é‡ä¸ºç»ˆæ­¢æ¡ä»¶ï¼Œè€Œéè½®æ¬¡æ•°é‡
"""

import asyncio
import json
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

from loguru import logger

from app.config import REQUIREMENT_QUALITY_CONFIG
from app.llm import LLM


class RequirementDimension(Enum):
    """éœ€æ±‚ç»´åº¦æšä¸¾"""

    FUNCTIONAL = "åŠŸèƒ½éœ€æ±‚"
    NON_FUNCTIONAL = "éåŠŸèƒ½éœ€æ±‚"
    USER_ROLES = "ç”¨æˆ·è§’è‰²"
    BUSINESS_RULES = "ä¸šåŠ¡è§„åˆ™"
    CONSTRAINTS = "çº¦æŸæ¡ä»¶"
    ACCEPTANCE_CRITERIA = "éªŒæ”¶æ ‡å‡†"
    INTEGRATION = "é›†æˆéœ€æ±‚"
    DATA_REQUIREMENTS = "æ•°æ®éœ€æ±‚"


class MissingAspectPriority(Enum):
    """ç¼ºå¤±æ–¹é¢ä¼˜å…ˆçº§"""

    CRITICAL = "å…³é”®"  # å¿…é¡»æ¾„æ¸…ï¼Œå¦åˆ™æ— æ³•è¿›è¡Œ
    HIGH = "é‡è¦"  # å¼ºçƒˆå»ºè®®æ¾„æ¸…ï¼Œå½±å“æ ¸å¿ƒåŠŸèƒ½
    MEDIUM = "ä¸€èˆ¬"  # å»ºè®®æ¾„æ¸…ï¼Œå½±å“ç”¨æˆ·ä½“éªŒ
    LOW = "å¯é€‰"  # å¯ä»¥æš‚ç¼“ï¼Œå±äºä¼˜åŒ–é¡¹


class ImpactScope(Enum):
    """å½±å“èŒƒå›´"""

    CORE_BUSINESS = "æ ¸å¿ƒä¸šåŠ¡"  # å½±å“æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
    USER_EXPERIENCE = "ç”¨æˆ·ä½“éªŒ"  # å½±å“ç”¨æˆ·ä½¿ç”¨ä½“éªŒ
    SYSTEM_QUALITY = "ç³»ç»Ÿè´¨é‡"  # å½±å“ç³»ç»Ÿè´¨é‡å±æ€§
    MAINTENANCE = "ç»´æŠ¤æ€§"  # å½±å“åæœŸç»´æŠ¤


class RiskLevel(Enum):
    """é£é™©çº§åˆ«"""

    HIGH_RISK = "é«˜é£é™©"  # å¯èƒ½å¯¼è‡´é¡¹ç›®å¤±è´¥
    MEDIUM_RISK = "ä¸­é£é™©"  # å¯èƒ½å¯¼è‡´è¿”å·¥
    LOW_RISK = "ä½é£é™©"  # å½±å“æœ‰é™


@dataclass
class DimensionQuality:
    """ç»´åº¦è´¨é‡è¯„ä¼°"""

    dimension: RequirementDimension
    completeness: float  # å®Œæ•´æ€§ 0-1
    clarity: float  # æ¸…æ™°åº¦ 0-1
    specificity: float  # å…·ä½“æ€§ 0-1
    feasibility: float  # å¯è¡Œæ€§ 0-1
    overall_score: float  # ç»¼åˆè¯„åˆ† 0-1
    missing_aspects: List[str]  # ç¼ºå¤±æ–¹é¢
    improvement_suggestions: List[str]  # æ”¹è¿›å»ºè®®


@dataclass
class ClarificationGoal:
    """æ¾„æ¸…ç›®æ ‡"""

    dimension: RequirementDimension
    target_quality: float  # ç›®æ ‡è´¨é‡åˆ†æ•°
    key_questions: List[str]  # å…³é”®é—®é¢˜
    priority: int  # ä¼˜å…ˆçº§ 1-5
    estimated_effort: str  # é¢„ä¼°æ¾„æ¸…éš¾åº¦


@dataclass
class MissingAspectClassification:
    """ç¼ºå¤±æ–¹é¢åˆ†ç±»"""

    aspect: str  # ç¼ºå¤±æ–¹é¢æè¿°
    priority: MissingAspectPriority  # ä¼˜å…ˆçº§
    impact_scope: ImpactScope  # å½±å“èŒƒå›´
    risk_level: RiskLevel  # é£é™©çº§åˆ«
    business_impact: float  # ä¸šåŠ¡å½±å“åº¦ (0-1)
    clarification_effort: str  # æ¾„æ¸…éš¾åº¦ (ç®€å•/ä¸­ç­‰/å¤æ‚)
    suggested_questions: List[str]  # å»ºè®®æ¾„æ¸…é—®é¢˜
    rationale: str  # åˆ†çº§ç†ç”±


class QualityDrivenClarificationEngine:
    """è´¨é‡å¯¼å‘çš„æ¾„æ¸…å¼•æ“"""

    def __init__(self):
        """
        åˆå§‹åŒ–è´¨é‡é©±åŠ¨æ¾„æ¸…å¼•æ“
        ä½¿ç”¨é…ç½®åŒ–å‚æ•°ï¼Œé¿å…ç¡¬ç¼–ç 
        """
        from app.llm import LLM

        self.llm = LLM()

        # ä»é…ç½®åŠ è½½å‚æ•°
        self.config = REQUIREMENT_QUALITY_CONFIG

        # è´¨é‡é˜ˆå€¼ï¼ˆåŸºäºéœ€æ±‚å·¥ç¨‹æœ€ä½³å®è·µï¼‰
        self.quality_threshold = self.config["quality_thresholds"]["overall_threshold"]
        self.dimension_threshold = self.config["quality_thresholds"][
            "dimension_threshold"
        ]
        self.excellent_threshold = self.config["quality_thresholds"][
            "excellent_threshold"
        ]

        # æ¾„æ¸…è½®æ¬¡æ§åˆ¶
        self.max_rounds = self.config["clarification_rounds"]["max_rounds"]
        self.min_rounds = self.config["clarification_rounds"]["min_rounds"]
        self.early_stop_threshold = self.config["clarification_rounds"][
            "early_stop_threshold"
        ]

        # ç»´åº¦æƒé‡ï¼ˆåŸºäºè½¯ä»¶å·¥ç¨‹ç†è®ºï¼‰
        self.dimension_weights = self.config["dimension_weights"]

        # è¯„åˆ†ä¸¥æ ¼åº¦é…ç½®
        self.scoring_config = self.config["scoring_strictness"]

    async def analyze_requirement_quality(
        self, requirement_text: str, clarification_history: List[Dict] = None
    ) -> Dict[RequirementDimension, DimensionQuality]:
        """
        åˆ†æéœ€æ±‚è´¨é‡ - é€†å‘æ€ç»´ï¼šä»æœ€ç»ˆæ–‡æ¡£è´¨é‡è¦æ±‚å€’æ¨
        """
        logger.info("ğŸ¯ å¼€å§‹è´¨é‡å¯¼å‘çš„éœ€æ±‚åˆ†æ...")

        # æ„å»ºè´¨é‡åˆ†ææç¤ºè¯
        analysis_prompt = self._build_quality_analysis_prompt(
            requirement_text, clarification_history
        )

        # å¹¶è¡Œåˆ†ææ‰€æœ‰ç»´åº¦
        dimension_tasks = []
        for dimension in RequirementDimension:
            task = self._analyze_single_dimension(
                dimension, requirement_text, clarification_history
            )
            dimension_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
        dimension_results = await asyncio.gather(*dimension_tasks)

        # æ„å»ºè´¨é‡è¯„ä¼°ç»“æœ
        quality_assessment = {}
        for i, dimension in enumerate(RequirementDimension):
            quality_assessment[dimension] = dimension_results[i]

        # è®°å½•è´¨é‡è¯„ä¼°æ—¥å¿—
        overall_quality = self._calculate_overall_quality(quality_assessment)
        logger.info(f"ğŸ“Š éœ€æ±‚æ•´ä½“è´¨é‡è¯„åˆ†: {overall_quality:.2f}")

        return quality_assessment

    async def _analyze_single_dimension(
        self,
        dimension: RequirementDimension,
        requirement_text: str,
        clarification_history: List[Dict] = None,
    ) -> DimensionQuality:
        """åˆ†æå•ä¸ªéœ€æ±‚ç»´åº¦çš„è´¨é‡"""

        prompt = f"""ä½œä¸ºéœ€æ±‚åˆ†æä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹éœ€æ±‚åœ¨ã€{dimension.value}ã€‘ç»´åº¦çš„è´¨é‡ï¼š

éœ€æ±‚æ–‡æœ¬ï¼š"{requirement_text}"

æ¾„æ¸…å†å²ï¼š{clarification_history if clarification_history else "æ— "}

è¯·ä»ä»¥ä¸‹è§’åº¦è¯„ä¼°ï¼ˆåŠ¡å®æ ‡å‡†ï¼Œä¸è¦è¿‡äºè‹›åˆ»ï¼‰ï¼š

1. å®Œæ•´æ€§ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: æ ¸å¿ƒä¿¡æ¯åŸºæœ¬é½å…¨
   - 0.5-0.7: ä¸»è¦å†…å®¹æ˜ç¡®ï¼Œéƒ¨åˆ†ç»†èŠ‚å¾…è¡¥å……
   - 0.3-0.5: åŸºæœ¬æ¦‚å¿µæ¸…æ¥šï¼Œç¼ºå°‘é‡è¦ç»†èŠ‚
   - 0.0-0.3: ä¿¡æ¯ä¸¥é‡ä¸è¶³

2. æ¸…æ™°åº¦ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: è¡¨è¿°æ¸…æ™°æ˜“æ‡‚
   - 0.5-0.7: å¤§éƒ¨åˆ†å†…å®¹æ¸…æ¥š
   - 0.3-0.5: åŸºæœ¬èƒ½ç†è§£ï¼Œéœ€è¦æ¾„æ¸…
   - 0.0-0.3: è¡¨è¿°æ¨¡ç³Šéš¾æ‡‚

3. å…·ä½“æ€§ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: æœ‰å…·ä½“çš„æ“ä½œæè¿°
   - 0.5-0.7: æœ‰ä¸€å®šå…·ä½“æè¿°
   - 0.3-0.5: åæ¦‚å¿µåŒ–ï¼Œç¼ºå°‘ç»†èŠ‚
   - 0.0-0.3: è¿‡äºæŠ½è±¡ç¬¼ç»Ÿ

4. å¯è¡Œæ€§ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: æ˜ç¡®å¯è¡Œ
   - 0.5-0.7: åŸºæœ¬å¯è¡Œ
   - 0.3-0.5: æœ‰æŒ‘æˆ˜ä½†å¯èƒ½å®ç°
   - 0.0-0.3: å­˜åœ¨æ˜æ˜¾æŠ€æœ¯éš¾é¢˜

æ³¨æ„ï¼šæ­£å¸¸çš„ä¸šåŠ¡éœ€æ±‚æè¿°é€šå¸¸åº”è¯¥è·å¾—0.6-0.8çš„è¯„åˆ†ï¼Œåªæœ‰ç‰¹åˆ«è¯¦ç»†çš„æ‰èƒ½è¾¾åˆ°0.9+ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "completeness": 0.7,
    "clarity": 0.8,
    "specificity": 0.6,
    "feasibility": 0.9,
    "missing_aspects": ["å…·ä½“çš„åŠŸèƒ½è¾¹ç•Œ", "æ€§èƒ½æŒ‡æ ‡"],
    "improvement_suggestions": ["éœ€è¦æ˜ç¡®å…·ä½“çš„åŠŸèƒ½èŒƒå›´", "åº”è¯¥æä¾›é‡åŒ–çš„æ€§èƒ½è¦æ±‚"]
}}"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                stream=False,
            )

            result = self._parse_json_response(response)

            # è®¡ç®—ç»¼åˆè¯„åˆ† - æ¢å¤ä¸¥è°¨çš„è¯„åˆ†ç®—æ³•
            completeness = result.get("completeness", 0.3)  # é»˜è®¤å€¼é™ä½
            clarity = result.get("clarity", 0.3)
            specificity = result.get("specificity", 0.3)
            feasibility = result.get("feasibility", 0.3)

            # ä½¿ç”¨é…ç½®åŒ–çš„æƒé‡
            weights = [
                self.scoring_config["completeness_weight"],
                self.scoring_config["clarity_weight"],
                self.scoring_config["specificity_weight"],
                self.scoring_config["feasibility_weight"],
            ]
            scores = [completeness, clarity, specificity, feasibility]

            # åŸºç¡€è¯„åˆ†ï¼šåŠ æƒå¹³å‡
            overall_score = sum(
                score * weight for score, weight in zip(scores, weights)
            )

            # ä¸¥æ ¼çš„å¥–åŠ±æœºåˆ¶ï¼šåªæœ‰åœ¨æ‰€æœ‰ç»´åº¦éƒ½è¾¾åˆ°è¾ƒé«˜æ°´å¹³æ—¶æ‰ç»™å¥–åŠ±
            bonus_threshold = self.scoring_config["bonus_threshold"]
            bonus_multiplier = self.scoring_config["bonus_multiplier"]

            if all(score >= bonus_threshold for score in scores):
                overall_score = min(overall_score * bonus_multiplier, 1.0)

            # è´¨é‡æƒ©ç½šï¼šå¦‚æœæœ‰ç»´åº¦ä¸¥é‡ä¸è¶³ï¼Œæ•´ä½“è¯„åˆ†åº”è¯¥å—å½±å“
            min_score = min(scores)
            if min_score < 0.4:  # å¦‚æœæœ‰ç»´åº¦è¯„åˆ†è¿‡ä½
                overall_score = min(overall_score, min_score + 0.3)  # é™åˆ¶æ€»åˆ†

            # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
            overall_score = max(0.0, min(1.0, overall_score))

            return DimensionQuality(
                dimension=dimension,
                completeness=completeness,
                clarity=clarity,
                specificity=specificity,
                feasibility=feasibility,
                overall_score=overall_score,
                missing_aspects=result.get("missing_aspects", []),
                improvement_suggestions=result.get("improvement_suggestions", []),
            )

        except Exception as e:
            logger.error(f"åˆ†æç»´åº¦ {dimension.value} è´¨é‡å¤±è´¥: {e}")
            # å°è¯•ç®€åŒ–æç¤ºè¯é‡è¯•ä¸€æ¬¡
            try:
                logger.info(f"å°è¯•ç®€åŒ–æç¤ºè¯é‡æ–°åˆ†æ {dimension.value}")
                simple_prompt = f"""è¯·ç®€å•è¯„ä¼°ä»¥ä¸‹éœ€æ±‚åœ¨ã€{dimension.value}ã€‘ç»´åº¦çš„è´¨é‡ï¼ˆ0-1åˆ†ï¼‰ï¼š

éœ€æ±‚æ–‡æœ¬ï¼š"{requirement_text}"

è¿”å›JSONæ ¼å¼ï¼š
{{
    "completeness": 0.5,
    "clarity": 0.5,
    "specificity": 0.5,
    "feasibility": 0.5,
    "missing_aspects": ["éœ€è¦è¡¥å……çš„ä¿¡æ¯"],
    "improvement_suggestions": ["æ”¹è¿›å»ºè®®"]
}}"""

                response = await self.llm.ask(
                    messages=[{"role": "user", "content": simple_prompt}],
                    temperature=0.1,
                    stream=False,
                )

                result = self._parse_json_response(response)

                # è®¡ç®—ç»¼åˆè¯„åˆ†
                overall_score = (
                    result.get("completeness", 0.5) * 0.3
                    + result.get("clarity", 0.5) * 0.25
                    + result.get("specificity", 0.5) * 0.25
                    + result.get("feasibility", 0.5) * 0.2
                )

                return DimensionQuality(
                    dimension=dimension,
                    completeness=result.get("completeness", 0.5),
                    clarity=result.get("clarity", 0.5),
                    specificity=result.get("specificity", 0.5),
                    feasibility=result.get("feasibility", 0.5),
                    overall_score=overall_score,
                    missing_aspects=result.get(
                        "missing_aspects", ["åˆ†æå¼‚å¸¸ï¼Œéœ€è¦é‡æ–°è¯„ä¼°"]
                    ),
                    improvement_suggestions=result.get(
                        "improvement_suggestions", ["å»ºè®®é‡æ–°åˆ†æè¯¥ç»´åº¦"]
                    ),
                )

            except Exception as retry_error:
                logger.error(f"é‡è¯•åˆ†æç»´åº¦ {dimension.value} ä»ç„¶å¤±è´¥: {retry_error}")
                return DimensionQuality(
                    dimension=dimension,
                    completeness=0.5,
                    clarity=0.5,
                    specificity=0.5,
                    feasibility=0.5,
                    overall_score=0.5,
                    missing_aspects=["åˆ†æå¤±è´¥"],
                    improvement_suggestions=["éœ€è¦é‡æ–°åˆ†æè¯¥ç»´åº¦"],
                )

    def _calculate_overall_quality(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> float:
        """
        è®¡ç®—æ•´ä½“è´¨é‡è¯„åˆ†

        ä¼˜åŒ–æƒé‡åˆ†é…ï¼šåŠŸèƒ½éœ€æ±‚å’Œç”¨æˆ·è§’è‰²æƒé‡æ›´é«˜
        """
        if not quality_assessment:
            return 0.0

        # è°ƒæ•´ç»´åº¦æƒé‡ - è®©æ ¸å¿ƒç»´åº¦æƒé‡æ›´é«˜
        dimension_weights = {
            RequirementDimension.FUNCTIONAL: 0.25,  # åŠŸèƒ½éœ€æ±‚æœ€é‡è¦
            RequirementDimension.USER_ROLES: 0.20,  # ç”¨æˆ·è§’è‰²å¾ˆé‡è¦
            RequirementDimension.ACCEPTANCE_CRITERIA: 0.15,  # éªŒæ”¶æ ‡å‡†é‡è¦
            RequirementDimension.NON_FUNCTIONAL: 0.12,  # éåŠŸèƒ½éœ€æ±‚
            RequirementDimension.BUSINESS_RULES: 0.10,  # ä¸šåŠ¡è§„åˆ™
            RequirementDimension.CONSTRAINTS: 0.08,  # çº¦æŸæ¡ä»¶
            RequirementDimension.INTEGRATION: 0.05,  # é›†æˆéœ€æ±‚
            RequirementDimension.DATA_REQUIREMENTS: 0.05,  # æ•°æ®éœ€æ±‚
        }

        weighted_scores = []
        total_weight = 0.0

        for dimension, quality in quality_assessment.items():
            weight = dimension_weights.get(dimension, 0.1)  # é»˜è®¤æƒé‡0.1
            weighted_score = quality.overall_score * weight
            weighted_scores.append(weighted_score)
            total_weight += weight

        # ç¡®ä¿æƒé‡æ€»å’Œä¸º1
        if total_weight > 0:
            overall_quality = sum(weighted_scores) / total_weight
        else:
            overall_quality = sum(
                q.overall_score for q in quality_assessment.values()
            ) / len(quality_assessment)

        return min(overall_quality, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1.0

    async def generate_targeted_clarification_goals(
        self, quality_assessment: Dict, requirement_text: str = ""
    ) -> List[str]:
        """
        åŸºäºè´¨é‡è¯„ä¼°ç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„æ¾„æ¸…ç›®æ ‡

        æ–°ç­–ç•¥ï¼šä½¿ç”¨åˆ†çº§å¤„ç†æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†å…³é”®å’Œé‡è¦çš„ç¼ºå¤±æ–¹é¢
        """
        clarification_goals = []

        # ä½¿ç”¨ä¼ å…¥çš„éœ€æ±‚æ–‡æœ¬ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤å€¼
        if not requirement_text:
            requirement_text = "å¾…è·å–çš„éœ€æ±‚æ–‡æœ¬"

        # ä½¿ç”¨åˆ†çº§å¤„ç†æœºåˆ¶
        try:
            classified_aspects = await self.classify_missing_aspects(
                requirement_text, quality_assessment
            )

            # æŒ‰åˆ†çº§ç­–ç•¥ç”Ÿæˆæ¾„æ¸…ç›®æ ‡
            processed_count = 0

            # 1. é¦–å…ˆå¤„ç†å…³é”®(CRITICAL)ç¼ºå¤±æ–¹é¢
            for classification in classified_aspects:
                if (
                    classification.priority == MissingAspectPriority.CRITICAL
                    and processed_count < 3
                ):  # æ¯è½®æœ€å¤š3ä¸ªå…³é”®é—®é¢˜
                    goal = f"ğŸš¨ã€å…³é”®-{classification.impact_scope.value}ã€‘{classification.aspect}"
                    clarification_goals.append(goal)
                    processed_count += 1

            # 2. ç„¶åå¤„ç†é‡è¦(HIGH)ç¼ºå¤±æ–¹é¢
            for classification in classified_aspects:
                if (
                    classification.priority == MissingAspectPriority.HIGH
                    and processed_count < 5
                ):  # æ€»å…±æœ€å¤š5ä¸ªé—®é¢˜
                    goal = f"ğŸ”´ã€é‡è¦-{classification.impact_scope.value}ã€‘{classification.aspect}"
                    clarification_goals.append(goal)
                    processed_count += 1

            # 3. å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œå¤„ç†ä¸€èˆ¬(MEDIUM)ç¼ºå¤±æ–¹é¢ï¼ˆä»…é™æ ¸å¿ƒä¸šåŠ¡å½±å“ï¼‰
            for classification in classified_aspects:
                if (
                    classification.priority == MissingAspectPriority.MEDIUM
                    and classification.impact_scope == ImpactScope.CORE_BUSINESS
                    and processed_count < 5
                ):
                    goal = f"ğŸŸ¡ã€ä¸€èˆ¬-æ ¸å¿ƒä¸šåŠ¡ã€‘{classification.aspect}"
                    clarification_goals.append(goal)
                    processed_count += 1

        except Exception as e:
            logger.error(f"åˆ†çº§å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {e}")
            # é™çº§å¤„ç†ï¼šç›´æ¥ä»è´¨é‡åˆ†æä¸­æå–
            for dimension_name, analysis in quality_assessment.items():
                if isinstance(analysis, dict) and "missing_aspects" in analysis:
                    missing_aspects = analysis["missing_aspects"]
                    if missing_aspects:
                        for aspect in missing_aspects[:2]:
                            goal = f"ã€{dimension_name}ã€‘{aspect}"
                            clarification_goals.append(goal)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†çº§çš„ç¼ºå¤±æ–¹é¢ï¼Œç”Ÿæˆé»˜è®¤ç›®æ ‡
        if not clarification_goals:
            clarification_goals = [
                "ğŸš¨ã€å…³é”®-æ ¸å¿ƒä¸šåŠ¡ã€‘æ˜ç¡®ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½å’Œæ“ä½œæµç¨‹",
                "ğŸ”´ã€é‡è¦-ç”¨æˆ·ä½“éªŒã€‘å®šä¹‰ç”¨æˆ·è§’è‰²å’Œæƒé™åˆ†é…",
                "ğŸŸ¡ã€ä¸€èˆ¬-ç³»ç»Ÿè´¨é‡ã€‘åˆ¶å®šå…·ä½“çš„æ€§èƒ½æŒ‡æ ‡å’ŒéªŒæ”¶æ ‡å‡†",
            ]

        return clarification_goals[:5]  # æœ€å¤š5ä¸ªç›®æ ‡

    def _calculate_goal_oriented_score(
        self, analysis_result: dict, clarification_history: List[dict]
    ) -> float:
        """
        è®¡ç®—ç›®æ ‡å¯¼å‘è¯„åˆ† - ä¿®æ­£ç‰ˆ
        è¯„ä¼°æ˜¯å¦æœ‰æ•ˆè¾¾æˆæœ€ç»ˆç›®æ ‡ï¼šç”Ÿæˆé«˜è´¨é‡éœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦

        ä¸å†ä»¥è½®æ¬¡æ•°é‡ä¸ºæŒ‡æ ‡ï¼Œè€Œæ˜¯è¯„ä¼°ç›®æ ‡è¾¾æˆçš„æœ‰æ•ˆæ€§
        """
        goal_achievement_score = 0.0

        # 1. æœ€ç»ˆç›®æ ‡å®Œæˆåº¦è¯„ä¼° (40%)
        final_quality = analysis_result.get("final_quality_score", 0.0)
        goal_completion = min(final_quality / 0.8, 1.0)  # ç›®æ ‡æ˜¯è¾¾åˆ°0.8è´¨é‡
        goal_achievement_score += goal_completion * 0.4

        # 2. ç›®æ ‡è¾¾æˆæ•ˆç‡è¯„ä¼° (30%)
        # è¯„ä¼°è¾¾æˆç›®æ ‡çš„æ•ˆç‡ï¼Œè€Œéå•çº¯çš„è½®æ¬¡å¤šå°‘
        total_rounds = len(clarification_history)
        if total_rounds > 0:
            quality_improvement_per_round = final_quality / total_rounds
            efficiency_score = min(quality_improvement_per_round * 5, 1.0)  # å½’ä¸€åŒ–
            goal_achievement_score += efficiency_score * 0.3

        # 3. ç›®æ ‡ä¸€è‡´æ€§è¯„ä¼° (20%)
        # è¯„ä¼°æ•´ä¸ªè¿‡ç¨‹æ˜¯å¦å§‹ç»ˆå›´ç»•æœ€ç»ˆç›®æ ‡è¿›è¡Œ
        consistency_score = self._evaluate_goal_consistency(clarification_history)
        goal_achievement_score += consistency_score * 0.2

        # 4. ç›®æ ‡ä»·å€¼å®ç°è¯„ä¼° (10%)
        # è¯„ä¼°æ˜¯å¦å®ç°äº†ç”¨æˆ·çš„çœŸå®ç›®æ ‡ä»·å€¼
        value_realization = self._evaluate_value_realization(analysis_result)
        goal_achievement_score += value_realization * 0.1

        return min(goal_achievement_score, 1.0)

    def _evaluate_goal_consistency(self, clarification_history: List[dict]) -> float:
        """è¯„ä¼°ç›®æ ‡ä¸€è‡´æ€§ï¼šæ•´ä¸ªè¿‡ç¨‹æ˜¯å¦å§‹ç»ˆå›´ç»•æœ€ç»ˆç›®æ ‡"""
        if not clarification_history:
            return 0.0

        consistency_indicators = []

        for record in clarification_history:
            questions = record.get("questions", [])
            # è¯„ä¼°é—®é¢˜æ˜¯å¦éƒ½æŒ‡å‘éœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦çš„å…³é”®è¦ç´ 
            relevant_questions = 0
            total_questions = len(questions)

            for question in questions:
                # å®‰å…¨æå–é—®é¢˜æ–‡æœ¬
                if isinstance(question, dict):
                    question_text = question.get("question", "").lower()
                elif isinstance(question, str):
                    question_text = question.lower()
                else:
                    question_text = str(question).lower()

                # æ£€æŸ¥é—®é¢˜æ˜¯å¦é’ˆå¯¹éœ€æ±‚æ–‡æ¡£çš„æ ¸å¿ƒè¦ç´ 
                core_elements = [
                    "åŠŸèƒ½éœ€æ±‚",
                    "éåŠŸèƒ½éœ€æ±‚",
                    "ç”¨æˆ·è§’è‰²",
                    "ä¸šåŠ¡è§„åˆ™",
                    "çº¦æŸæ¡ä»¶",
                    "éªŒæ”¶æ ‡å‡†",
                    "æŠ€æœ¯æ–¹æ¡ˆ",
                    "å®ç°æ–¹å¼",
                ]
                if any(element in question_text for element in core_elements):
                    relevant_questions += 1

            if total_questions > 0:
                consistency_indicators.append(relevant_questions / total_questions)

        return (
            sum(consistency_indicators) / len(consistency_indicators)
            if consistency_indicators
            else 0.0
        )

    def _evaluate_value_realization(self, analysis_result: dict) -> float:
        """è¯„ä¼°ä»·å€¼å®ç°ï¼šæ˜¯å¦å®ç°äº†ç”¨æˆ·çš„çœŸå®ç›®æ ‡ä»·å€¼"""
        value_score = 0.0

        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº†éœ€æ±‚æ–‡æ¡£çš„å…³é”®éƒ¨åˆ†
        key_components = [
            "functional_requirements",  # åŠŸèƒ½éœ€æ±‚
            "non_functional_requirements",  # éåŠŸèƒ½éœ€æ±‚
            "user_stories",  # ç”¨æˆ·æ•…äº‹
            "acceptance_criteria",  # éªŒæ”¶æ ‡å‡†
            "technical_constraints",  # æŠ€æœ¯çº¦æŸ
            "business_rules",  # ä¸šåŠ¡è§„åˆ™
        ]

        present_components = 0
        for component in key_components:
            if component in analysis_result:
                present_components += 1

        value_score = present_components / len(key_components)

        return value_score

    async def generate_clarification_questions(
        self, clarification_goals: List[str]
    ) -> List[str]:
        """
        åŸºäºæ¾„æ¸…ç›®æ ‡ç”Ÿæˆç²¾å‡†çš„é—®é¢˜

        æ–°ç­–ç•¥ï¼šæ ¹æ®è´¨é‡åˆ†æçš„"ç¼ºå¤±æ–¹é¢"ç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜
        """
        if not clarification_goals:
            return ["è¯·æä¾›æ›´è¯¦ç»†çš„éœ€æ±‚è¯´æ˜ï¼Œä»¥ä¾¿è¿›è¡Œæ·±å…¥åˆ†æã€‚"]

        # æ„å»ºæ™ºèƒ½æç¤ºè¯ï¼Œé’ˆå¯¹ç¼ºå¤±æ–¹é¢ç”Ÿæˆé—®é¢˜
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆï¼ŒåŸºäºè´¨é‡åˆ†æç»“æœç”Ÿæˆç²¾å‡†çš„æ¾„æ¸…é—®é¢˜ã€‚

## å½“å‰éœ€è¦æ¾„æ¸…çš„ç›®æ ‡ï¼š
{chr(10).join([f"- {goal}" for goal in clarification_goals])}

## è¦æ±‚ï¼š
1. æ¯ä¸ªé—®é¢˜å¿…é¡»é’ˆå¯¹å…·ä½“çš„ç¼ºå¤±æ–¹é¢
2. é—®é¢˜è¦å…·ä½“ã€å¯æ“ä½œã€æœ‰æ˜ç¡®çš„å›ç­”æ–¹å‘
3. é¿å…æ¨¡ç³Šç¬¼ç»Ÿçš„é—®é¢˜
4. ä¼˜å…ˆå…³æ³¨åŠŸèƒ½éœ€æ±‚ã€ç”¨æˆ·è§’è‰²ã€éªŒæ”¶æ ‡å‡†ç­‰å…³é”®ç»´åº¦
5. é—®é¢˜è¦å¼•å¯¼ç”¨æˆ·æä¾›é‡åŒ–ã€å…·ä½“çš„ä¿¡æ¯

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ç›´æ¥è¾“å‡º3-4ä¸ªæ¾„æ¸…é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ã€‚

ç¤ºä¾‹æ ¼å¼ï¼š
1. å­¦ç”Ÿä¿¡æ¯ç®¡ç†åŠŸèƒ½å…·ä½“åŒ…å«å“ªäº›æ“ä½œï¼Ÿè¯·åˆ—å‡ºå¢åˆ æ”¹æŸ¥çš„è¯¦ç»†æµç¨‹ã€‚
2. ç³»ç»Ÿéœ€è¦æ”¯æŒå¤šå°‘å¹¶å‘ç”¨æˆ·ï¼Ÿè¯·æä¾›å…·ä½“çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚200ç”¨æˆ·åŒæ—¶ç™»å½•ï¼‰ã€‚
3. è¯·æ˜ç¡®æ¯ä¸ªç”¨æˆ·è§’è‰²çš„å…·ä½“æƒé™ï¼Œä¾‹å¦‚æ•™å¸ˆæ˜¯å¦å¯ä»¥ä¿®æ”¹å­¦ç”ŸåŸºæœ¬ä¿¡æ¯ï¼Ÿ
"""

        try:
            from app.llm import LLM
            from app.schema import Message

            llm = LLM()
            messages = [Message.user_message(prompt)]

            response = await llm.ask(
                messages=messages,
                temperature=0.3,
                stream=False,
            )

            # è§£æé—®é¢˜
            questions = []
            for line in response.strip().split("\n"):
                line = line.strip()
                if line and len(line) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„è¡Œ
                    # ç§»é™¤åºå·å‰ç¼€
                    import re

                    line = re.sub(r"^\d+\.\s*", "", line)
                    line = re.sub(r"^[-*]\s*", "", line)
                    if line:
                        questions.append(line)

            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªé—®é¢˜
            if not questions:
                questions = ["è¯·æä¾›æ›´è¯¦ç»†çš„éœ€æ±‚ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…·ä½“çš„åŠŸèƒ½è¦æ±‚å’Œæ€§èƒ½æŒ‡æ ‡ã€‚"]

            return questions[:4]  # æœ€å¤š4ä¸ªé—®é¢˜

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¾„æ¸…é—®é¢˜å¤±è´¥: {e}")
            # é™çº§å¤„ç†
            return [
                "è¯·è¯¦ç»†æè¿°ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬å…·ä½“çš„æ“ä½œæµç¨‹ã€‚",
                "è¯·æ˜ç¡®ç”¨æˆ·è§’è‰²å’Œæƒé™åˆ†é…çš„å…·ä½“è¦æ±‚ã€‚",
                "è¯·æä¾›ç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡è¦æ±‚ï¼Œå¦‚å¹¶å‘ç”¨æˆ·æ•°ã€å“åº”æ—¶é—´ç­‰ã€‚",
            ]

    def generate_quality_report(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> str:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š"""

        overall_quality = self._calculate_overall_quality(quality_assessment)

        report = f"""
# éœ€æ±‚è´¨é‡è¯„ä¼°æŠ¥å‘Š

## ğŸ“Š æ•´ä½“è´¨é‡è¯„åˆ†: {overall_quality:.2f}/1.0

## ğŸ“‹ å„ç»´åº¦è¯¦ç»†è¯„ä¼°:

"""

        for dimension, quality in quality_assessment.items():
            status = "âœ…" if quality.overall_score >= self.dimension_threshold else "âš ï¸"
            report += f"""
### {status} {dimension.value} (è¯„åˆ†: {quality.overall_score:.2f})
- **å®Œæ•´æ€§**: {quality.completeness:.2f}
- **æ¸…æ™°åº¦**: {quality.clarity:.2f}
- **å…·ä½“æ€§**: {quality.specificity:.2f}
- **å¯è¡Œæ€§**: {quality.feasibility:.2f}

**ç¼ºå¤±æ–¹é¢**: {', '.join(quality.missing_aspects) if quality.missing_aspects else 'æ— '}
**æ”¹è¿›å»ºè®®**: {', '.join(quality.improvement_suggestions) if quality.improvement_suggestions else 'æ— '}
"""

        # æ·»åŠ æ€»ä½“å»ºè®®
        if overall_quality >= self.quality_threshold:
            report += "\n## ğŸ‰ æ€»ä½“å»ºè®®: éœ€æ±‚è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚"
        else:
            report += f"\n## ğŸ” æ€»ä½“å»ºè®®: éœ€æ±‚è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨è¯„åˆ†ä½äº{self.dimension_threshold}çš„ç»´åº¦ã€‚"

        return report

    def _build_quality_analysis_prompt(
        self, requirement_text: str, clarification_history: List[Dict] = None
    ) -> str:
        """æ„å»ºè´¨é‡åˆ†ææç¤ºè¯"""
        return f"""ä½œä¸ºéœ€æ±‚è´¨é‡åˆ†æä¸“å®¶ï¼Œè¯·å…¨é¢è¯„ä¼°ä»¥ä¸‹éœ€æ±‚çš„è´¨é‡çŠ¶å†µï¼š

éœ€æ±‚æè¿°: "{requirement_text}"

æ¾„æ¸…å†å²: {clarification_history if clarification_history else "æ— "}

è¯·ä»è½¯ä»¶å·¥ç¨‹éœ€æ±‚åˆ†æçš„è§’åº¦ï¼Œç³»ç»Ÿæ€§è¯„ä¼°éœ€æ±‚åœ¨å„ä¸ªç»´åº¦çš„è´¨é‡æ°´å¹³ã€‚"""

    def _parse_json_response(self, response: str) -> Dict:
        """è§£æJSONå“åº” - å¢å¼ºå®¹é”™æ€§"""
        import json
        import re

        if not response or not response.strip():
            logger.warning("LLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return self._get_default_response()

        # æ¸…ç†å“åº”å­—ç¬¦ä¸²
        cleaned_response = response.strip()

        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(cleaned_response)
        except json.JSONDecodeError:
            try:
                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                if cleaned_response.startswith("```json"):
                    cleaned_response = (
                        cleaned_response.replace("```json", "")
                        .replace("```", "")
                        .strip()
                    )
                elif cleaned_response.startswith("```"):
                    cleaned_response = cleaned_response.replace("```", "").strip()

                # å†æ¬¡å°è¯•è§£æ
                return json.loads(cleaned_response)
            except json.JSONDecodeError:
                try:
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    json_match = re.search(r"\{.*\}", cleaned_response, re.DOTALL)
                    if json_match:
                        json_text = json_match.group()
                        return json.loads(json_text)
                    else:
                        logger.error(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆJSON: {cleaned_response[:200]}...")
                        return self._get_default_response()
                except json.JSONDecodeError as e:
                    logger.error(
                        f"JSONè§£ææœ€ç»ˆå¤±è´¥: {e}, å“åº”å†…å®¹: {cleaned_response[:200]}..."
                    )
                    return self._get_default_response()

    def _get_default_response(self) -> Dict:
        """è¿”å›é»˜è®¤å“åº”ç»“æ„"""
        return {
            "completeness": 0.5,
            "clarity": 0.5,
            "specificity": 0.5,
            "feasibility": 0.5,
            "missing_aspects": ["åˆ†æå“åº”å¼‚å¸¸"],
            "improvement_suggestions": ["éœ€è¦é‡æ–°åˆ†æ"],
            "questions": ["è¯·æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯"],
        }

    async def should_continue_clarification(
        self, quality_analysis: Dict, current_round: int, requirement_text: str = ""
    ) -> tuple:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­æ¾„æ¸…

        åŸºäºç§‘å­¦çš„è´¨é‡æ ‡å‡†ã€è®¾è®¡å°±ç»ªåº¦å’Œæœ€å°‘æ¾„æ¸…è½®æ¬¡è¦æ±‚
        """
        overall_quality = self._calculate_overall_quality(quality_analysis)

        # ä½¿ç”¨é…ç½®åŒ–çš„é˜ˆå€¼
        overall_threshold = self.quality_threshold
        dimension_threshold = self.dimension_threshold
        max_rounds = self.max_rounds
        min_rounds = self.min_rounds
        design_threshold = self.config["quality_thresholds"][
            "design_readiness_threshold"
        ]

        # 1. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
        if current_round >= max_rounds:
            return False, f"ğŸ”„ å·²è¾¾åˆ°æœ€å¤§æ¾„æ¸…è½®æ¬¡({max_rounds}è½®)"

        # 2. æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°‘æ¾„æ¸…è½®æ¬¡è¦æ±‚
        if current_round < min_rounds:
            return (
                True,
                f"ğŸ“ éœ€è¦å®Œæˆæœ€å°‘{min_rounds}è½®æ¾„æ¸…ä»¥ç¡®ä¿è´¨é‡ï¼ˆå½“å‰ç¬¬{current_round}è½®ï¼‰",
            )

        # 3. æ£€æŸ¥æ•´ä½“è´¨é‡
        if overall_quality < overall_threshold:
            return (
                True,
                f"ğŸ“Š æ•´ä½“è´¨é‡ä¸è¾¾æ ‡: {overall_quality:.2f} < {overall_threshold}",
            )

        # 4. æ£€æŸ¥å„ç»´åº¦è´¨é‡
        low_quality_dimensions = []
        for dim_name, analysis in quality_analysis.items():
            if isinstance(analysis, dict) and "score" in analysis:
                if analysis["score"] < dimension_threshold:
                    low_quality_dimensions.append(
                        f"{dim_name}: {analysis['score']:.2f}"
                    )

        if low_quality_dimensions:
            return True, f"ğŸ” å…³é”®ç»´åº¦è´¨é‡ä¸è¾¾æ ‡: {', '.join(low_quality_dimensions)}"

        # 5. è¯„ä¼°è®¾è®¡å°±ç»ªåº¦
        if requirement_text:
            try:
                design_readiness = await self.assess_design_readiness(
                    requirement_text, quality_analysis
                )
                readiness_score = design_readiness.get("overall_readiness", 0)

                if readiness_score < design_threshold:
                    blocking_issues = design_readiness.get("blocking_issues", [])
                    return (
                        True,
                        f"ğŸ¯ è®¾è®¡å°±ç»ªåº¦ä¸è¶³: {readiness_score:.2f} < {design_threshold}ï¼Œé˜»å¡é—®é¢˜: {', '.join(blocking_issues[:3])}",
                    )

                logger.info(
                    f"âœ… è®¾è®¡å°±ç»ªåº¦è¾¾æ ‡: {readiness_score:.2f} ({design_readiness.get('readiness_level', 'æœªçŸ¥')})"
                )
            except Exception as e:
                logger.warning(f"è®¾è®¡å°±ç»ªåº¦è¯„ä¼°å¼‚å¸¸ï¼Œç»§ç»­æ¾„æ¸…: {e}")
                return True, f"âš ï¸ è®¾è®¡å°±ç»ªåº¦è¯„ä¼°å¼‚å¸¸ï¼Œå»ºè®®ç»§ç»­æ¾„æ¸…"

        # 6. å†²çªåˆ†æ ğŸ”¥ æ–°å¢
        try:
            conflict_analysis = await self.analyze_knowledge_and_code_conflicts(
                requirement_text, quality_analysis
            )

            conflict_level = conflict_analysis.get("overall_conflict_level", "unknown")
            critical_conflicts = conflict_analysis.get("critical_conflicts", [])

            if conflict_level == "critical" and len(critical_conflicts) > 0:
                conflict_descriptions = [
                    c.get("description", "")[:50] for c in critical_conflicts[:2]
                ]
                return (
                    True,
                    f"âš ï¸ å‘ç°ä¸¥é‡å†²çªéœ€è¦è§£å†³: {conflict_level}ï¼Œå†²çª: {', '.join(conflict_descriptions)}",
                )

            if conflict_analysis.get("requires_stakeholder_decision", False):
                return (
                    True,
                    f"ğŸ¤ éœ€è¦åˆ©ç›Šç›¸å…³è€…å†³ç­–: å‘ç°{len(critical_conflicts)}ä¸ªå…³é”®å†²çª",
                )

            logger.info(
                f"ğŸ” å†²çªåˆ†æå®Œæˆ: {conflict_level}çº§åˆ«ï¼Œ{len(critical_conflicts)}ä¸ªä¸¥é‡å†²çª"
            )

        except Exception as e:
            logger.warning(f"å†²çªåˆ†æå¼‚å¸¸ï¼Œä½†ä¸é˜»å¡æµç¨‹: {e}")
            # å†²çªåˆ†æå¼‚å¸¸ä¸åº”è¯¥é˜»å¡æ•´ä¸ªæµç¨‹

        # 7. æ‰€æœ‰æ¡ä»¶æ»¡è¶³ï¼Œå¯ä»¥ç»“æŸæ¾„æ¸…
        return (
            False,
            f"âœ… éœ€æ±‚è´¨é‡è¾¾æ ‡ä¸”è®¾è®¡å°±ç»ªï¼Œæ— ä¸¥é‡å†²çªï¼Œæ•´ä½“è¯„åˆ†: {overall_quality:.2f}",
        )

    async def classify_missing_aspects(
        self, requirement_text: str, quality_analysis: Dict
    ) -> List[MissingAspectClassification]:
        """
        æ™ºèƒ½åˆ†çº§å¤„ç†éœ€æ±‚ç¼ºå¤±æ–¹é¢

        åŸºäºä¸šåŠ¡å½±å“ã€é£é™©çº§åˆ«ã€æ¾„æ¸…éš¾åº¦ç­‰å¤šç»´åº¦è¿›è¡Œåˆ†çº§
        """
        classifications = []

        # æ”¶é›†æ‰€æœ‰ç»´åº¦çš„ç¼ºå¤±æ–¹é¢
        all_missing_aspects = []
        for dim_name, analysis in quality_analysis.items():
            if isinstance(analysis, dict) and "missing_aspects" in analysis:
                for aspect in analysis["missing_aspects"]:
                    all_missing_aspects.append(
                        {
                            "dimension": dim_name,
                            "aspect": aspect,
                            "dimension_score": analysis.get("score", 0.5),
                        }
                    )

        # å¯¹æ¯ä¸ªç¼ºå¤±æ–¹é¢è¿›è¡Œæ™ºèƒ½åˆ†çº§
        for missing_item in all_missing_aspects:
            classification = await self._analyze_missing_aspect_priority(
                missing_item, requirement_text
            )
            classifications.append(classification)

        # æŒ‰ä¼˜å…ˆçº§å’Œä¸šåŠ¡å½±å“æ’åº
        classifications.sort(
            key=lambda x: (
                self._get_priority_weight(x.priority),
                x.business_impact,
                self._get_risk_weight(x.risk_level),
            ),
            reverse=True,
        )

        return classifications

    async def _analyze_missing_aspect_priority(
        self, missing_item: Dict, requirement_text: str
    ) -> MissingAspectClassification:
        """
        åˆ†æå•ä¸ªç¼ºå¤±æ–¹é¢çš„ä¼˜å…ˆçº§å’Œåˆ†ç±»
        """
        dimension = missing_item["dimension"]
        aspect = missing_item["aspect"]

        # æ„å»ºæ™ºèƒ½åˆ†çº§æç¤ºè¯
        prompt = f"""ä½œä¸ºéœ€æ±‚åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç¼ºå¤±æ–¹é¢è¿›è¡Œä¸“ä¸šåˆ†çº§åˆ†æï¼š

## èƒŒæ™¯ä¿¡æ¯
éœ€æ±‚æè¿°: "{requirement_text}"
éœ€æ±‚ç»´åº¦: {dimension}
ç¼ºå¤±æ–¹é¢: {aspect}

## åˆ†çº§ä»»åŠ¡
è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°è¿™ä¸ªç¼ºå¤±æ–¹é¢ï¼š

1. ä¼˜å…ˆçº§è¯„ä¼° (å…³é”®/é‡è¦/ä¸€èˆ¬/å¯é€‰):
   - å…³é”®: ä¸æ¾„æ¸…å°±æ— æ³•ç»§ç»­å¼€å‘
   - é‡è¦: å½±å“æ ¸å¿ƒåŠŸèƒ½ï¼Œå¼ºçƒˆå»ºè®®æ¾„æ¸…
   - ä¸€èˆ¬: å½±å“ç”¨æˆ·ä½“éªŒï¼Œå»ºè®®æ¾„æ¸…
   - å¯é€‰: ä¼˜åŒ–é¡¹ï¼Œå¯ä»¥æš‚ç¼“

2. å½±å“èŒƒå›´ (æ ¸å¿ƒä¸šåŠ¡/ç”¨æˆ·ä½“éªŒ/ç³»ç»Ÿè´¨é‡/ç»´æŠ¤æ€§):
   - æ ¸å¿ƒä¸šåŠ¡: ç›´æ¥å½±å“ä¸»è¦ä¸šåŠ¡æµç¨‹
   - ç”¨æˆ·ä½“éªŒ: å½±å“ç”¨æˆ·ä½¿ç”¨æ„Ÿå—
   - ç³»ç»Ÿè´¨é‡: å½±å“æ€§èƒ½ã€å®‰å…¨ã€å¯é æ€§
   - ç»´æŠ¤æ€§: å½±å“åæœŸç»´æŠ¤å’Œæ‰©å±•

3. é£é™©çº§åˆ« (é«˜é£é™©/ä¸­é£é™©/ä½é£é™©):
   - é«˜é£é™©: å¯èƒ½å¯¼è‡´é¡¹ç›®å¤±è´¥æˆ–é‡å¤§è¿”å·¥
   - ä¸­é£é™©: å¯èƒ½éœ€è¦å±€éƒ¨è°ƒæ•´
   - ä½é£é™©: å½±å“æœ‰é™

4. ä¸šåŠ¡å½±å“åº¦ (0-1çš„æ•°å€¼):
   è¯„ä¼°å¯¹æ•´ä½“ä¸šåŠ¡ç›®æ ‡å®ç°çš„å½±å“ç¨‹åº¦

5. æ¾„æ¸…éš¾åº¦ (ç®€å•/ä¸­ç­‰/å¤æ‚):
   è¯„ä¼°æ¾„æ¸…è¿™ä¸ªæ–¹é¢éœ€è¦çš„å·¥ä½œé‡

## è¾“å‡ºæ ¼å¼
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "priority": "å…³é”®/é‡è¦/ä¸€èˆ¬/å¯é€‰",
    "impact_scope": "æ ¸å¿ƒä¸šåŠ¡/ç”¨æˆ·ä½“éªŒ/ç³»ç»Ÿè´¨é‡/ç»´æŠ¤æ€§",
    "risk_level": "é«˜é£é™©/ä¸­é£é™©/ä½é£é™©",
    "business_impact": 0.85,
    "clarification_effort": "ç®€å•/ä¸­ç­‰/å¤æ‚",
    "suggested_questions": [
        "å…·ä½“çš„æ¾„æ¸…é—®é¢˜1",
        "å…·ä½“çš„æ¾„æ¸…é—®é¢˜2"
    ],
    "rationale": "åˆ†çº§ç†ç”±çš„è¯¦ç»†è¯´æ˜"
}}"""

        try:
            from app.llm import LLM
            from app.schema import Message

            llm = LLM()
            messages = [Message.user_message(prompt)]

            response = await llm.ask(
                messages=messages,
                temperature=0.2,  # ä¿æŒä¸€è‡´æ€§
                stream=False,
            )

            # è§£æLLMå“åº”
            analysis_result = self._parse_json_response(response)

            # æ˜ å°„åˆ°æšä¸¾ç±»å‹
            priority_map = {
                "å…³é”®": MissingAspectPriority.CRITICAL,
                "é‡è¦": MissingAspectPriority.HIGH,
                "ä¸€èˆ¬": MissingAspectPriority.MEDIUM,
                "å¯é€‰": MissingAspectPriority.LOW,
            }

            impact_map = {
                "æ ¸å¿ƒä¸šåŠ¡": ImpactScope.CORE_BUSINESS,
                "ç”¨æˆ·ä½“éªŒ": ImpactScope.USER_EXPERIENCE,
                "ç³»ç»Ÿè´¨é‡": ImpactScope.SYSTEM_QUALITY,
                "ç»´æŠ¤æ€§": ImpactScope.MAINTENANCE,
            }

            risk_map = {
                "é«˜é£é™©": RiskLevel.HIGH_RISK,
                "ä¸­é£é™©": RiskLevel.MEDIUM_RISK,
                "ä½é£é™©": RiskLevel.LOW_RISK,
            }

            return MissingAspectClassification(
                aspect=aspect,
                priority=priority_map.get(
                    analysis_result.get("priority", "ä¸€èˆ¬"),
                    MissingAspectPriority.MEDIUM,
                ),
                impact_scope=impact_map.get(
                    analysis_result.get("impact_scope", "ç”¨æˆ·ä½“éªŒ"),
                    ImpactScope.USER_EXPERIENCE,
                ),
                risk_level=risk_map.get(
                    analysis_result.get("risk_level", "ä¸­é£é™©"), RiskLevel.MEDIUM_RISK
                ),
                business_impact=float(analysis_result.get("business_impact", 0.5)),
                clarification_effort=analysis_result.get(
                    "clarification_effort", "ä¸­ç­‰"
                ),
                suggested_questions=analysis_result.get("suggested_questions", []),
                rationale=analysis_result.get("rationale", "é»˜è®¤åˆ†çº§"),
            )

        except Exception as e:
            logger.error(f"ç¼ºå¤±æ–¹é¢åˆ†çº§åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†çº§
            return MissingAspectClassification(
                aspect=aspect,
                priority=MissingAspectPriority.MEDIUM,
                impact_scope=ImpactScope.USER_EXPERIENCE,
                risk_level=RiskLevel.MEDIUM_RISK,
                business_impact=0.5,
                clarification_effort="ä¸­ç­‰",
                suggested_questions=[f"è¯·è¯¦ç»†è¯´æ˜{aspect}çš„å…·ä½“è¦æ±‚"],
                rationale="åˆ†æå¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤åˆ†çº§",
            )

    def _get_priority_weight(self, priority: MissingAspectPriority) -> int:
        """è·å–ä¼˜å…ˆçº§æƒé‡"""
        weights = {
            MissingAspectPriority.CRITICAL: 4,
            MissingAspectPriority.HIGH: 3,
            MissingAspectPriority.MEDIUM: 2,
            MissingAspectPriority.LOW: 1,
        }
        return weights.get(priority, 2)

    def _get_risk_weight(self, risk: RiskLevel) -> int:
        """è·å–é£é™©æƒé‡"""
        weights = {
            RiskLevel.HIGH_RISK: 3,
            RiskLevel.MEDIUM_RISK: 2,
            RiskLevel.LOW_RISK: 1,
        }
        return weights.get(risk, 2)

    async def assess_design_readiness(
        self, requirement_text: str, quality_analysis: Dict
    ) -> Dict:
        """
        è¯„ä¼°éœ€æ±‚çš„è®¾è®¡å°±ç»ªåº¦
        åŸºäº"èƒ½å¦æ”¯æ’‘åç»­è®¾è®¡å’Œå¼€å‘"çš„æ ‡å‡†
        """
        logger.info("ğŸ¯ è¯„ä¼°éœ€æ±‚è®¾è®¡å°±ç»ªåº¦...")

        # è¯„ä¼°å„ä¸ªå¯è®¾è®¡æ€§ç»´åº¦
        readiness_prompt = f"""ä½œä¸ºè½¯ä»¶æ¶æ„å¸ˆï¼Œè¯·è¯„ä¼°ä»¥ä¸‹éœ€æ±‚æ˜¯å¦è¶³ä»¥æ”¯æ’‘åç»­çš„ç³»ç»Ÿè®¾è®¡å’Œå¼€å‘ï¼š

éœ€æ±‚æ–‡æœ¬ï¼š"{requirement_text}"

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦è¯„ä¼°ï¼ˆ0-1åˆ†ï¼‰ï¼š

1. **æ¶æ„è®¾è®¡å¯è¡Œæ€§** (0-1)ï¼š
   - èƒ½å¦åŸºäºæ­¤éœ€æ±‚è®¾è®¡ç³»ç»Ÿæ¶æ„ï¼Ÿ
   - éåŠŸèƒ½éœ€æ±‚æ˜¯å¦æ˜ç¡®ï¼Ÿ
   - æŠ€æœ¯é€‰å‹æ˜¯å¦æœ‰æŒ‡å¯¼æ„ä¹‰ï¼Ÿ
   è¯„åˆ†æ ‡å‡†ï¼š
   - 0.8-1.0: å¯ä»¥ç›´æ¥è¿›è¡Œæ¶æ„è®¾è®¡
   - 0.6-0.8: åŸºæœ¬å¯è¡Œï¼Œéœ€è¦å°‘é‡è¡¥å……
   - 0.4-0.6: éœ€è¦å¤§é‡è¡¥å……æ‰èƒ½è®¾è®¡
   - 0.0-0.4: æ— æ³•è¿›è¡Œæ¶æ„è®¾è®¡

2. **å®ç°æŒ‡å¯¼æ¸…æ™°æ€§** (0-1)ï¼š
   - å¼€å‘äººå‘˜èƒ½å¦åŸºäºæ­¤éœ€æ±‚ç¼–ç ï¼Ÿ
   - åŠŸèƒ½è¾¹ç•Œæ˜¯å¦æ¸…æ™°ï¼Ÿ
   - ä¸šåŠ¡è§„åˆ™æ˜¯å¦å®Œæ•´ï¼Ÿ
   è¯„åˆ†æ ‡å‡†ï¼š
   - 0.8-1.0: å¯ä»¥ç›´æ¥æŒ‡å¯¼å¼€å‘
   - 0.6-0.8: åŸºæœ¬æ¸…æ™°ï¼Œéœ€è¦å°‘é‡æ¾„æ¸…
   - 0.4-0.6: éœ€è¦å¤§é‡æ¾„æ¸…
   - 0.0-0.4: æ— æ³•æŒ‡å¯¼å¼€å‘

3. **æµ‹è¯•å®Œæ•´æ€§** (0-1)ï¼š
   - èƒ½å¦åŸºäºæ­¤éœ€æ±‚åˆ¶å®šæµ‹è¯•ç”¨ä¾‹ï¼Ÿ
   - éªŒæ”¶æ¡ä»¶æ˜¯å¦å¯é‡åŒ–ï¼Ÿ
   - å¼‚å¸¸åœºæ™¯æ˜¯å¦è€ƒè™‘ï¼Ÿ
   è¯„åˆ†æ ‡å‡†ï¼š
   - 0.8-1.0: å¯ä»¥åˆ¶å®šå®Œæ•´æµ‹è¯•ç”¨ä¾‹
   - 0.6-0.8: å¯ä»¥åˆ¶å®šåŸºæœ¬æµ‹è¯•ç”¨ä¾‹
   - 0.4-0.6: æµ‹è¯•ç”¨ä¾‹ä¸å®Œæ•´
   - 0.0-0.4: æ— æ³•åˆ¶å®šæµ‹è¯•ç”¨ä¾‹

4. **é¡¹ç›®å¯æ§æ€§** (0-1)ï¼š
   - å·¥ä½œé‡æ˜¯å¦å¯ä¼°ç®—ï¼Ÿ
   - é£é™©æ˜¯å¦å¯è¯†åˆ«ï¼Ÿ
   - é‡Œç¨‹ç¢‘æ˜¯å¦å¯åˆ¶å®šï¼Ÿ
   è¯„åˆ†æ ‡å‡†ï¼š
   - 0.8-1.0: é¡¹ç›®å®Œå…¨å¯æ§
   - 0.6-0.8: åŸºæœ¬å¯æ§
   - 0.4-0.6: éƒ¨åˆ†å¯æ§
   - 0.0-0.4: é¡¹ç›®ä¸å¯æ§

5. **é£é™©è¯†åˆ«åº¦** (0-1)ï¼š
   - æŠ€æœ¯é£é™©æ˜¯å¦å¯è¯†åˆ«ï¼Ÿ
   - ä¸šåŠ¡é£é™©æ˜¯å¦æ¸…æ¥šï¼Ÿ
   - ä¾èµ–å…³ç³»æ˜¯å¦æ˜ç¡®ï¼Ÿ

è¿”å›JSONæ ¼å¼ï¼š
{{
    "architecture_feasibility": 0.7,
    "implementation_clarity": 0.6,
    "testing_completeness": 0.5,
    "project_controllability": 0.4,
    "risk_identification": 0.6,
    "overall_readiness": 0.58,
    "readiness_level": "ä¸è¶³",
    "blocking_issues": ["ç¼ºå°‘éåŠŸèƒ½éœ€æ±‚", "ä¸šåŠ¡è§„åˆ™ä¸æ˜ç¡®"],
    "next_actions": ["æ¾„æ¸…æ€§èƒ½è¦æ±‚", "æ˜ç¡®ä¸šåŠ¡æµç¨‹"]
}}"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": readiness_prompt}],
                temperature=0.1,
                stream=False,
            )

            result = self._parse_json_response(response)

            # è®¡ç®—ç»¼åˆè®¾è®¡å°±ç»ªåº¦
            design_config = self.config["design_readiness"]
            overall_readiness = (
                result.get("architecture_feasibility", 0)
                * design_config["architecture_feasibility"]
                + result.get("implementation_clarity", 0)
                * design_config["implementation_clarity"]
                + result.get("testing_completeness", 0)
                * design_config["testing_completeness"]
                + result.get("project_controllability", 0)
                * design_config["project_controllability"]
                + result.get("risk_identification", 0)
                * design_config["risk_identification"]
            )

            # ç¡®å®šå°±ç»ªçº§åˆ«
            if overall_readiness >= 0.90:
                readiness_level = "ä¼˜ç§€"
            elif overall_readiness >= 0.80:
                readiness_level = "è‰¯å¥½"
            elif overall_readiness >= 0.70:
                readiness_level = "åŸºæœ¬è¾¾æ ‡"
            elif overall_readiness >= 0.60:
                readiness_level = "éœ€è¦æ”¹è¿›"
            else:
                readiness_level = "ä¸¥é‡ä¸è¶³"

            result["overall_readiness"] = overall_readiness
            result["readiness_level"] = readiness_level

            logger.info(
                f"ğŸ¯ è®¾è®¡å°±ç»ªåº¦è¯„ä¼°: {overall_readiness:.2f} ({readiness_level})"
            )

            return result

        except Exception as e:
            logger.error(f"è®¾è®¡å°±ç»ªåº¦è¯„ä¼°å¤±è´¥: {e}")
            return {
                "architecture_feasibility": 0.3,
                "implementation_clarity": 0.3,
                "testing_completeness": 0.3,
                "project_controllability": 0.3,
                "risk_identification": 0.3,
                "overall_readiness": 0.3,
                "readiness_level": "è¯„ä¼°å¤±è´¥",
                "blocking_issues": ["è¯„ä¼°ç³»ç»Ÿå¼‚å¸¸"],
                "next_actions": ["é‡æ–°è¯„ä¼°"],
            }

    async def analyze_knowledge_and_code_conflicts(
        self, requirement_text: str, quality_analysis: Dict
    ) -> Dict:
        """
        åˆ†æéœ€æ±‚ä¸çŸ¥è¯†åº“ã€ä»£ç åº“çš„å†²çª
        å®ç°æ™ºèƒ½åŒ–çš„å†²çªæ£€æµ‹å’Œå·®å¼‚å¤„ç†
        """
        logger.info("ğŸ” å¼€å§‹çŸ¥è¯†åº“å’Œä»£ç åº“å†²çªåˆ†æ...")

        try:
            # 1. çŸ¥è¯†åº“å†²çªåˆ†æ
            knowledge_conflicts = await self._analyze_knowledge_conflicts(
                requirement_text
            )

            # 2. ä»£ç åº“å†²çªåˆ†æ
            codebase_conflicts = await self._analyze_codebase_conflicts(
                requirement_text
            )

            # 3. ç»¼åˆå†²çªè¯„ä¼°
            conflict_summary = self._synthesize_conflict_analysis(
                knowledge_conflicts, codebase_conflicts, requirement_text
            )

            logger.info(
                f"ğŸ” å†²çªåˆ†æå®Œæˆï¼Œå‘ç° {len(conflict_summary.get('critical_conflicts', []))} ä¸ªä¸¥é‡å†²çª"
            )

            return conflict_summary

        except Exception as e:
            logger.error(f"å†²çªåˆ†æå¤±è´¥: {e}")
            return {
                "knowledge_conflicts": [],
                "codebase_conflicts": [],
                "critical_conflicts": [],
                "manageable_differences": [],
                "conflict_resolution_suggestions": [],
                "overall_conflict_level": "unknown",
            }

    async def _analyze_knowledge_conflicts(self, requirement_text: str) -> Dict:
        """åˆ†æä¸çŸ¥è¯†åº“çš„å†²çª"""

        # è°ƒç”¨çŸ¥è¯†åº“æœåŠ¡æœç´¢ç›¸å…³çŸ¥è¯†
        try:
            from app.modules.knowledge_base.service import KnowledgeService
            from app.modules.knowledge_base.types import KnowledgeQuery, KnowledgeType

            knowledge_service = KnowledgeService()

            # æœç´¢ç›¸å…³çŸ¥è¯†ï¼ˆä½¿ç”¨æ­£ç¡®çš„APIï¼‰
            knowledge_query = KnowledgeQuery(
                query_text=requirement_text,
                knowledge_types=[
                    KnowledgeType.REQUIREMENTS_TEMPLATES,
                    KnowledgeType.BEST_PRACTICES,
                ],
                min_confidence=0.5,
                limit=10,
            )
            search_results = knowledge_service.search_knowledge(knowledge_query)

            # æ„å»ºçŸ¥è¯†åº“åˆ†ææç¤º
            relevant_knowledge = ""
            if (
                search_results
                and hasattr(search_results, "results")
                and search_results.results
            ):
                relevant_knowledge = "\n".join(
                    [
                        f"- {result.entry.title}: {result.entry.summary}"
                        for result in search_results.results[:5]
                    ]
                )
            else:
                relevant_knowledge = "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†åº“å†…å®¹"

        except Exception as e:
            logger.warning(f"çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}")
            relevant_knowledge = "çŸ¥è¯†åº“æŸ¥è¯¢å¼‚å¸¸"

        knowledge_prompt = f"""ä½œä¸ºçŸ¥è¯†ç®¡ç†ä¸“å®¶ï¼Œè¯·åˆ†æç”¨æˆ·éœ€æ±‚ä¸å·²æœ‰çŸ¥è¯†åº“çš„å†²çªæƒ…å†µï¼š

ç”¨æˆ·éœ€æ±‚ï¼š"{requirement_text}"

ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼š
{relevant_knowledge}

è¯·åˆ†æä»¥ä¸‹ç±»å‹çš„å†²çªï¼š

1. **ç¡¬å†²çª**ï¼ˆä¸å¯æ¥å—çš„å†²çªï¼‰ï¼š
   - è¿åæ—¢å®šæ ‡å‡†æˆ–è§„èŒƒ
   - ä¸æ ¸å¿ƒæ¶æ„åŸåˆ™å†²çª
   - ä¸å®‰å…¨è¦æ±‚ç›¸å†²çª

2. **è½¯å†²çª**ï¼ˆå¯åå•†çš„å·®å¼‚ï¼‰ï¼š
   - ä¸ç°æœ‰æœ€ä½³å®è·µä¸åŒ
   - éœ€è¦æ–°çš„æŠ€æœ¯æ–¹æ¡ˆ
   - è¶…å‡ºç°æœ‰ç»éªŒèŒƒå›´

3. **åˆ›æ–°éœ€æ±‚**ï¼ˆåˆç†çš„æ–°éœ€æ±‚ï¼‰ï¼š
   - ä¸šåŠ¡å‘å±•çš„æ–°è¦æ±‚
   - æŠ€æœ¯æ¼”è¿›çš„æ–°æœºä¼š
   - ç”¨æˆ·ä½“éªŒçš„æ–°æœŸæœ›

è¿”å›JSONæ ¼å¼ï¼š
{{
    "hard_conflicts": [
        {{
            "conflict_type": "å®‰å…¨å†²çª",
            "description": "éœ€æ±‚è¦æ±‚æ˜æ–‡å­˜å‚¨å¯†ç ï¼Œè¿åå®‰å…¨è§„èŒƒ",
            "severity": "critical",
            "knowledge_source": "å®‰å…¨æœ€ä½³å®è·µ",
            "resolution_required": true
        }}
    ],
    "soft_conflicts": [
        {{
            "conflict_type": "æŠ€æœ¯é€‰å‹å·®å¼‚",
            "description": "éœ€æ±‚å»ºè®®ä½¿ç”¨æ–°æŠ€æœ¯æ ˆï¼Œä¸ç°æœ‰æŠ€æœ¯ä¸åŒ",
            "severity": "medium",
            "knowledge_source": "æŠ€æœ¯æ ‡å‡†",
            "negotiable": true
        }}
    ],
    "innovation_opportunities": [
        {{
            "opportunity_type": "ä¸šåŠ¡åˆ›æ–°",
            "description": "AIè¾…åŠ©åŠŸèƒ½æ˜¯æ–°çš„ä¸šåŠ¡æœºä¼š",
            "potential_value": "high",
            "knowledge_gap": "éœ€è¦è¡¥å……AIç›¸å…³çŸ¥è¯†"
        }}
    ],
    "knowledge_coverage": 0.75,
    "recommendation": "é‡ç‚¹å…³æ³¨å®‰å…¨å†²çªï¼Œå…¶ä»–å·®å¼‚å¯é€šè¿‡åå•†è§£å†³"
}}"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": knowledge_prompt}],
                temperature=0.1,
                stream=False,
            )

            return self._parse_json_response(response)

        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“å†²çªåˆ†æå¤±è´¥: {e}")
            return {
                "hard_conflicts": [],
                "soft_conflicts": [],
                "innovation_opportunities": [],
                "knowledge_coverage": 0.0,
                "recommendation": "çŸ¥è¯†åº“åˆ†æå¼‚å¸¸",
            }

    async def _analyze_codebase_conflicts(self, requirement_text: str) -> Dict:
        """åˆ†æä¸ä»£ç åº“çš„å†²çª"""

        # è°ƒç”¨ä»£ç åˆ†æå™¨
        try:
            from app.assistants.requirements.code_analyzer import CodeAnalyzer

            code_analyzer = CodeAnalyzer()

            # åˆ†æä»£ç åº“ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œé¿å…ç³»ç»Ÿæ–‡ä»¶ï¼‰
            import os

            current_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            code_analyzer = CodeAnalyzer(project_root=current_dir)
            analysis_result = code_analyzer.analyze_codebase(["app/"])

            # æå–å…³é”®ä¿¡æ¯
            project_overview = analysis_result.get("project_overview", {})
            codebase_summary = f"""
ä»£ç åº“è§„æ¨¡: {project_overview.get('total_files', 0)} ä¸ªæ–‡ä»¶
ä¸»è¦è¯­è¨€: {', '.join(project_overview.get('languages', {}).keys())}
è¯†åˆ«ç»„ä»¶: {len(analysis_result.get('components', []))} ä¸ª
è®¾è®¡æ¨¡å¼: {', '.join([p.get('name', '') for p in analysis_result.get('patterns', [])][:3])}
"""

        except Exception as e:
            logger.warning(f"ä»£ç åº“åˆ†æå¤±è´¥: {e}")
            codebase_summary = "ä»£ç åº“åˆ†æå¼‚å¸¸"

        codebase_prompt = f"""ä½œä¸ºä»£ç æ¶æ„ä¸“å®¶ï¼Œè¯·åˆ†æç”¨æˆ·éœ€æ±‚ä¸ç°æœ‰ä»£ç åº“çš„å†²çªæƒ…å†µï¼š

ç”¨æˆ·éœ€æ±‚ï¼š"{requirement_text}"

ä»£ç åº“åˆ†æï¼š
{codebase_summary}

è¯·åˆ†æä»¥ä¸‹å†²çªç±»å‹ï¼š

1. **æ¶æ„å†²çª**ï¼ˆéœ€è¦é‡å¤§é‡æ„ï¼‰ï¼š
   - è¿åç°æœ‰æ¶æ„åŸåˆ™
   - éœ€è¦ç ´åæ€§å˜æ›´
   - ä¸æ ¸å¿ƒè®¾è®¡æ¨¡å¼å†²çª

2. **æŠ€æœ¯å€ºåŠ¡**ï¼ˆéœ€è¦ä»£ç æ”¹è¿›ï¼‰ï¼š
   - ç°æœ‰ä»£ç è´¨é‡é™åˆ¶
   - éœ€è¦é‡æ„ä¼˜åŒ–
   - æŠ€æœ¯æ ˆä¸å…¼å®¹

3. **æ‰©å±•æœºä¼š**ï¼ˆå¯æ‰©å±•å®ç°ï¼‰ï¼š
   - åŸºäºç°æœ‰æ¶æ„æ‰©å±•
   - å¤ç”¨ç°æœ‰ç»„ä»¶
   - æ¸è¿›å¼å®ç°

è¿”å›JSONæ ¼å¼ï¼š
{{
    "architecture_conflicts": [
        {{
            "conflict_type": "æ¨¡å—æ¶æ„å†²çª",
            "description": "éœ€æ±‚è¦æ±‚çš„åŠŸèƒ½ä¸ç°æœ‰æ¨¡å—è®¾è®¡å†²çª",
            "affected_modules": ["æ¨¡å—1", "æ¨¡å—2"],
            "refactoring_required": "major",
            "estimated_effort": "2-3ä¸ªæœˆ"
        }}
    ],
    "technical_debt": [
        {{
            "debt_type": "ä»£ç è´¨é‡é—®é¢˜",
            "description": "ç°æœ‰ä»£ç ç¼ºå°‘æµ‹è¯•è¦†ç›–",
            "impact_on_requirement": "å¢åŠ å¼€å‘é£é™©",
            "improvement_needed": "æ·»åŠ å•å…ƒæµ‹è¯•"
        }}
    ],
    "extension_opportunities": [
        {{
            "opportunity_type": "ç»„ä»¶å¤ç”¨",
            "description": "å¯ä»¥å¤ç”¨ç°æœ‰ç”¨æˆ·ç®¡ç†ç»„ä»¶",
            "reusable_components": ["UserManager", "AuthService"],
            "implementation_approach": "æ‰©å±•ç°æœ‰æ¥å£"
        }}
    ],
    "overall_compatibility": 0.70,
    "implementation_feasibility": "medium"
}}"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": codebase_prompt}],
                temperature=0.1,
                stream=False,
            )

            return self._parse_json_response(response)

        except Exception as e:
            logger.error(f"ä»£ç åº“å†²çªåˆ†æå¤±è´¥: {e}")
            return {
                "architecture_conflicts": [],
                "technical_debt": [],
                "extension_opportunities": [],
                "overall_compatibility": 0.0,
                "implementation_feasibility": "unknown",
            }

    def _synthesize_conflict_analysis(
        self, knowledge_conflicts: Dict, codebase_conflicts: Dict, requirement_text: str
    ) -> Dict:
        """ç»¼åˆå†²çªåˆ†æï¼Œç”Ÿæˆå¤„ç†å»ºè®®"""

        # æå–å…³é”®å†²çª
        critical_conflicts = []
        manageable_differences = []

        # å¤„ç†çŸ¥è¯†åº“å†²çª
        for conflict in knowledge_conflicts.get("hard_conflicts", []):
            critical_conflicts.append(
                {
                    "type": "knowledge",
                    "category": conflict.get("conflict_type", ""),
                    "description": conflict.get("description", ""),
                    "severity": conflict.get("severity", "medium"),
                    "source": "knowledge_base",
                }
            )

        for conflict in knowledge_conflicts.get("soft_conflicts", []):
            manageable_differences.append(
                {
                    "type": "knowledge",
                    "category": conflict.get("conflict_type", ""),
                    "description": conflict.get("description", ""),
                    "negotiable": conflict.get("negotiable", True),
                    "source": "knowledge_base",
                }
            )

        # å¤„ç†ä»£ç åº“å†²çª
        for conflict in codebase_conflicts.get("architecture_conflicts", []):
            if conflict.get("refactoring_required") == "major":
                critical_conflicts.append(
                    {
                        "type": "codebase",
                        "category": "architecture",
                        "description": conflict.get("description", ""),
                        "severity": "critical",
                        "source": "codebase",
                    }
                )
            else:
                manageable_differences.append(
                    {
                        "type": "codebase",
                        "category": "architecture",
                        "description": conflict.get("description", ""),
                        "refactoring_effort": conflict.get(
                            "estimated_effort", "unknown"
                        ),
                        "source": "codebase",
                    }
                )

        # ç”Ÿæˆè§£å†³å»ºè®®
        resolution_suggestions = self._generate_conflict_resolution_suggestions(
            critical_conflicts, manageable_differences
        )

        # ç¡®å®šæ•´ä½“å†²çªçº§åˆ«
        if len(critical_conflicts) > 0:
            overall_level = "critical"
        elif len(manageable_differences) > 3:
            overall_level = "medium"
        else:
            overall_level = "low"

        return {
            "knowledge_conflicts": knowledge_conflicts,
            "codebase_conflicts": codebase_conflicts,
            "critical_conflicts": critical_conflicts,
            "manageable_differences": manageable_differences,
            "conflict_resolution_suggestions": resolution_suggestions,
            "overall_conflict_level": overall_level,
            "total_conflicts": len(critical_conflicts) + len(manageable_differences),
            "requires_stakeholder_decision": len(critical_conflicts) > 0,
        }

    def _generate_conflict_resolution_suggestions(
        self, critical_conflicts: List, manageable_differences: List
    ) -> List[str]:
        """ç”Ÿæˆå†²çªè§£å†³å»ºè®®"""
        suggestions = []

        if critical_conflicts:
            suggestions.append("ğŸš¨ å‘ç°ä¸¥é‡å†²çªï¼Œéœ€è¦åˆ©ç›Šç›¸å…³è€…å†³ç­–")
            suggestions.append("ğŸ“‹ å»ºè®®å¬å¼€æŠ€æœ¯è¯„å®¡ä¼šè®®è®¨è®ºè§£å†³æ–¹æ¡ˆ")

        if any(c.get("type") == "knowledge" for c in critical_conflicts):
            suggestions.append("ğŸ“š éœ€è¦æ›´æ–°çŸ¥è¯†åº“æˆ–è°ƒæ•´éœ€æ±‚ä»¥ç¬¦åˆæœ€ä½³å®è·µ")

        if any(c.get("type") == "codebase" for c in critical_conflicts):
            suggestions.append("ğŸ”§ éœ€è¦æ¶æ„é‡æ„æˆ–å¯»æ‰¾æ›¿ä»£æŠ€æœ¯æ–¹æ¡ˆ")

        if manageable_differences:
            suggestions.append("ğŸ¤ å¯é€šè¿‡æŠ€æœ¯åå•†è§£å†³éƒ¨åˆ†å·®å¼‚")
            suggestions.append("ğŸ“ˆ å»ºè®®é‡‡ç”¨æ¸è¿›å¼å®ç°ç­–ç•¥")

        if not critical_conflicts and len(manageable_differences) <= 2:
            suggestions.append("âœ… å†²çªå¯æ§ï¼Œå¯ä»¥æ­£å¸¸æ¨è¿›éœ€æ±‚å®ç°")

        return suggestions
