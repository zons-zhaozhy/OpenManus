"""
è´¨é‡å¯¼å‘çš„æ¾„æ¸…å¼•æ“
åŸºäºç”¨æˆ·åé¦ˆé‡æ–°è®¾è®¡ï¼šç›®æ ‡å¯¼å‘ã€é€†å‘æ€ç»´ã€è´¨é‡ä¸ºæœ¬

æ ¸å¿ƒç†å¿µï¼š
1. ç›®æ ‡å¯¼å‘ï¼šæ˜ç¡®æ¯æ¬¡æ¾„æ¸…è¦è¾¾åˆ°çš„å…·ä½“ç›®æ ‡
2. é€†å‘æ€ç»´ï¼šä»æœ€ç»ˆéœ€æ±‚æ–‡æ¡£è´¨é‡å€’æ¨éœ€è¦æ¾„æ¸…çš„å†…å®¹
3. è´¨é‡ä¸ºæœ¬ï¼šä»¥éœ€æ±‚å®Œæ•´æ€§å’Œè´¨é‡ä¸ºç»ˆæ­¢æ¡ä»¶ï¼Œè€Œéè½®æ¬¡æ•°é‡
"""

import asyncio
import time
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

from app.llm import LLM
from app.logger import logger


class RequirementDimension(Enum):
    """éœ€æ±‚ç»´åº¦æšä¸¾"""

    FUNCTIONAL = "åŠŸèƒ½éœ€æ±‚"
    NON_FUNCTIONAL = "éåŠŸèƒ½éœ€æ±‚"
    USER_ROLES = "ç”¨æˆ·è§’è‰²"
    BUSINESS_RULES = "ä¸šåŠ¡è§„åˆ™"
    CONSTRAINTS = "çº¦æŸæ¡ä»¶"
    ACCEPTANCE_CRITERIA = "éªŒæ”¶æ ‡å‡†"
    INTEGRATION = "é›†æˆéœ€æ±‚"
    DATA_REQUIREMENTS = "æ•°æ®éœ€æ±‚"


@dataclass
class DimensionQuality:
    """ç»´åº¦è´¨é‡è¯„ä¼°"""

    dimension: RequirementDimension
    completeness: float  # å®Œæ•´æ€§ 0-1
    clarity: float  # æ¸…æ™°åº¦ 0-1
    specificity: float  # å…·ä½“æ€§ 0-1
    feasibility: float  # å¯è¡Œæ€§ 0-1
    overall_score: float  # ç»¼åˆè¯„åˆ† 0-1
    missing_aspects: List[str]  # ç¼ºå¤±æ–¹é¢
    improvement_suggestions: List[str]  # æ”¹è¿›å»ºè®®


@dataclass
class ClarificationGoal:
    """æ¾„æ¸…ç›®æ ‡"""

    dimension: RequirementDimension
    target_quality: float  # ç›®æ ‡è´¨é‡åˆ†æ•°
    key_questions: List[str]  # å…³é”®é—®é¢˜
    priority: int  # ä¼˜å…ˆçº§ 1-5
    estimated_effort: str  # é¢„ä¼°æ¾„æ¸…éš¾åº¦


class QualityDrivenClarificationEngine:
    """è´¨é‡å¯¼å‘çš„æ¾„æ¸…å¼•æ“"""

    def __init__(self):
        self.llm = LLM()
        self.quality_threshold = 0.8  # æ•´ä½“è´¨é‡é˜ˆå€¼
        self.dimension_threshold = 0.7  # å•ç»´åº¦è´¨é‡é˜ˆå€¼
        self.max_clarification_attempts = 10  # é˜²æ­¢æ— é™å¾ªç¯çš„æœ€å¤§å°è¯•æ¬¡æ•°

    async def analyze_requirement_quality(
        self, requirement_text: str, clarification_history: List[Dict] = None
    ) -> Dict[RequirementDimension, DimensionQuality]:
        """
        åˆ†æéœ€æ±‚è´¨é‡ - é€†å‘æ€ç»´ï¼šä»æœ€ç»ˆæ–‡æ¡£è´¨é‡è¦æ±‚å€’æ¨
        """
        logger.info("ğŸ¯ å¼€å§‹è´¨é‡å¯¼å‘çš„éœ€æ±‚åˆ†æ...")

        # æ„å»ºè´¨é‡åˆ†ææç¤ºè¯
        analysis_prompt = self._build_quality_analysis_prompt(
            requirement_text, clarification_history
        )

        # å¹¶è¡Œåˆ†ææ‰€æœ‰ç»´åº¦
        dimension_tasks = []
        for dimension in RequirementDimension:
            task = self._analyze_single_dimension(
                dimension, requirement_text, clarification_history
            )
            dimension_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
        dimension_results = await asyncio.gather(*dimension_tasks)

        # æ„å»ºè´¨é‡è¯„ä¼°ç»“æœ
        quality_assessment = {}
        for i, dimension in enumerate(RequirementDimension):
            quality_assessment[dimension] = dimension_results[i]

        # è®°å½•è´¨é‡è¯„ä¼°æ—¥å¿—
        overall_quality = self._calculate_overall_quality(quality_assessment)
        logger.info(f"ğŸ“Š éœ€æ±‚æ•´ä½“è´¨é‡è¯„åˆ†: {overall_quality:.2f}")

        return quality_assessment

    async def _analyze_single_dimension(
        self,
        dimension: RequirementDimension,
        requirement_text: str,
        clarification_history: List[Dict] = None,
    ) -> DimensionQuality:
        """åˆ†æå•ä¸ªéœ€æ±‚ç»´åº¦çš„è´¨é‡"""

        prompt = f"""ä½œä¸ºéœ€æ±‚åˆ†æä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹éœ€æ±‚åœ¨ã€{dimension.value}ã€‘ç»´åº¦çš„è´¨é‡ï¼š

éœ€æ±‚æ–‡æœ¬ï¼š"{requirement_text}"

æ¾„æ¸…å†å²ï¼š{clarification_history if clarification_history else "æ— "}

è¯·ä»ä»¥ä¸‹è§’åº¦è¯„ä¼°ï¼š
1. å®Œæ•´æ€§ï¼ˆ0-1ï¼‰ï¼šè¯¥ç»´åº¦ä¿¡æ¯æ˜¯å¦å®Œæ•´ï¼Ÿ
2. æ¸…æ™°åº¦ï¼ˆ0-1ï¼‰ï¼šè¡¨è¿°æ˜¯å¦æ¸…æ™°æ˜ç¡®ï¼Ÿ
3. å…·ä½“æ€§ï¼ˆ0-1ï¼‰ï¼šæ˜¯å¦å…·ä½“å¯å®æ–½ï¼Ÿ
4. å¯è¡Œæ€§ï¼ˆ0-1ï¼‰ï¼šæ˜¯å¦åœ¨æŠ€æœ¯å’Œä¸šåŠ¡ä¸Šå¯è¡Œï¼Ÿ

è¿˜éœ€è¦è¯†åˆ«ï¼š
- ç¼ºå¤±çš„å…³é”®æ–¹é¢
- å…·ä½“çš„æ”¹è¿›å»ºè®®

è¿”å›JSONæ ¼å¼ï¼š
{{
    "completeness": 0.7,
    "clarity": 0.8,
    "specificity": 0.6,
    "feasibility": 0.9,
    "missing_aspects": ["å…·ä½“çš„åŠŸèƒ½è¾¹ç•Œ", "æ€§èƒ½æŒ‡æ ‡"],
    "improvement_suggestions": ["éœ€è¦æ˜ç¡®å…·ä½“çš„åŠŸèƒ½èŒƒå›´", "åº”è¯¥æä¾›é‡åŒ–çš„æ€§èƒ½è¦æ±‚"]
}}"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                stream=False,
            )

            result = self._parse_json_response(response)

            # è®¡ç®—ç»¼åˆè¯„åˆ†
            overall_score = (
                result.get("completeness", 0) * 0.3
                + result.get("clarity", 0) * 0.25
                + result.get("specificity", 0) * 0.25
                + result.get("feasibility", 0) * 0.2
            )

            return DimensionQuality(
                dimension=dimension,
                completeness=result.get("completeness", 0),
                clarity=result.get("clarity", 0),
                specificity=result.get("specificity", 0),
                feasibility=result.get("feasibility", 0),
                overall_score=overall_score,
                missing_aspects=result.get("missing_aspects", []),
                improvement_suggestions=result.get("improvement_suggestions", []),
            )

        except Exception as e:
            logger.error(f"åˆ†æç»´åº¦ {dimension.value} è´¨é‡å¤±è´¥: {e}")
            return DimensionQuality(
                dimension=dimension,
                completeness=0.5,
                clarity=0.5,
                specificity=0.5,
                feasibility=0.5,
                overall_score=0.5,
                missing_aspects=["åˆ†æå¤±è´¥"],
                improvement_suggestions=["éœ€è¦é‡æ–°åˆ†æè¯¥ç»´åº¦"],
            )

    def _calculate_overall_quality(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> float:
        """è®¡ç®—æ•´ä½“è´¨é‡è¯„åˆ†"""
        if not quality_assessment:
            return 0.0

        # åŠ æƒè®¡ç®—ï¼ˆæ ¸å¿ƒç»´åº¦æƒé‡æ›´é«˜ï¼‰
        weights = {
            RequirementDimension.FUNCTIONAL: 0.25,
            RequirementDimension.NON_FUNCTIONAL: 0.15,
            RequirementDimension.USER_ROLES: 0.15,
            RequirementDimension.BUSINESS_RULES: 0.1,
            RequirementDimension.CONSTRAINTS: 0.1,
            RequirementDimension.ACCEPTANCE_CRITERIA: 0.15,
            RequirementDimension.INTEGRATION: 0.05,
            RequirementDimension.DATA_REQUIREMENTS: 0.05,
        }

        weighted_score = 0.0
        for dimension, quality in quality_assessment.items():
            weight = weights.get(dimension, 0.1)
            weighted_score += quality.overall_score * weight

        return weighted_score

    async def generate_targeted_clarification_goals(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> List[ClarificationGoal]:
        """
        ç›®æ ‡å¯¼å‘ï¼šç”Ÿæˆé’ˆå¯¹æ€§çš„æ¾„æ¸…ç›®æ ‡
        ä¼˜å…ˆå¤„ç†è´¨é‡æœ€ä½ã€å½±å“æœ€å¤§çš„ç»´åº¦
        """
        logger.info("ğŸ¯ ç”Ÿæˆç›®æ ‡å¯¼å‘çš„æ¾„æ¸…è®¡åˆ’...")

        clarification_goals = []

        # æŒ‰è´¨é‡è¯„åˆ†å’Œé‡è¦æ€§æ’åº
        sorted_dimensions = self._prioritize_dimensions(quality_assessment)

        for dimension, quality in sorted_dimensions:
            # åªæœ‰è´¨é‡ä½äºé˜ˆå€¼çš„ç»´åº¦æ‰éœ€è¦æ¾„æ¸…
            if quality.overall_score < self.dimension_threshold:
                goal = await self._create_clarification_goal(dimension, quality)
                clarification_goals.append(goal)

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        clarification_goals.sort(key=lambda x: x.priority, reverse=True)

        logger.info(f"ğŸ“‹ ç”Ÿæˆäº† {len(clarification_goals)} ä¸ªæ¾„æ¸…ç›®æ ‡")
        return clarification_goals

    def _prioritize_dimensions(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> List[Tuple[RequirementDimension, DimensionQuality]]:
        """ç»´åº¦ä¼˜å…ˆçº§æ’åº"""

        # é‡è¦æ€§æƒé‡
        importance_weights = {
            RequirementDimension.FUNCTIONAL: 5,
            RequirementDimension.USER_ROLES: 4,
            RequirementDimension.ACCEPTANCE_CRITERIA: 4,
            RequirementDimension.NON_FUNCTIONAL: 3,
            RequirementDimension.BUSINESS_RULES: 3,
            RequirementDimension.CONSTRAINTS: 2,
            RequirementDimension.DATA_REQUIREMENTS: 2,
            RequirementDimension.INTEGRATION: 1,
        }

        # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°ï¼ˆè´¨é‡è¶Šä½ã€é‡è¦æ€§è¶Šé«˜ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        priority_scores = []
        for dimension, quality in quality_assessment.items():
            importance = importance_weights.get(dimension, 1)
            # ä¼˜å…ˆçº§ = é‡è¦æ€§ * (1 - è´¨é‡åˆ†æ•°)
            priority_score = importance * (1 - quality.overall_score)
            priority_scores.append((priority_score, dimension, quality))

        # æŒ‰ä¼˜å…ˆçº§åˆ†æ•°é™åºæ’åº
        priority_scores.sort(key=lambda x: x[0], reverse=True)

        return [(item[1], item[2]) for item in priority_scores]

    async def _create_clarification_goal(
        self, dimension: RequirementDimension, quality: DimensionQuality
    ) -> ClarificationGoal:
        """åˆ›å»ºæ¾„æ¸…ç›®æ ‡"""

        # åŸºäºç¼ºå¤±æ–¹é¢ç”Ÿæˆå…³é”®é—®é¢˜
        key_questions = await self._generate_targeted_questions(dimension, quality)

        # è®¡ç®—ä¼˜å…ˆçº§
        priority = self._calculate_priority(dimension, quality)

        # ä¼°ç®—æ¾„æ¸…éš¾åº¦
        estimated_effort = self._estimate_clarification_effort(quality)

        return ClarificationGoal(
            dimension=dimension,
            target_quality=max(0.8, quality.overall_score + 0.3),  # ç›®æ ‡æå‡0.3åˆ†
            key_questions=key_questions,
            priority=priority,
            estimated_effort=estimated_effort,
        )

    async def _generate_targeted_questions(
        self, dimension: RequirementDimension, quality: DimensionQuality
    ) -> List[str]:
        """åŸºäºè´¨é‡ç¼ºé™·ç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜"""

        prompt = f"""åŸºäºéœ€æ±‚ç»´åº¦ã€{dimension.value}ã€‘çš„è´¨é‡åˆ†æç»“æœï¼Œç”Ÿæˆ3-5ä¸ªæœ€å…³é”®çš„æ¾„æ¸…é—®é¢˜ï¼š

è´¨é‡è¯„ä¼°ï¼š
- å®Œæ•´æ€§: {quality.completeness:.2f}
- æ¸…æ™°åº¦: {quality.clarity:.2f}
- å…·ä½“æ€§: {quality.specificity:.2f}
- å¯è¡Œæ€§: {quality.feasibility:.2f}

ç¼ºå¤±æ–¹é¢ï¼š{quality.missing_aspects}
æ”¹è¿›å»ºè®®ï¼š{quality.improvement_suggestions}

è¯·ç”Ÿæˆèƒ½å¤Ÿç›´æ¥è§£å†³è¿™äº›è´¨é‡é—®é¢˜çš„å…·ä½“é—®é¢˜ï¼Œè¦æ±‚ï¼š
1. é—®é¢˜è¦å…·ä½“ã€æœ‰é’ˆå¯¹æ€§
2. èƒ½å¤Ÿç›´æ¥è·å¾—å¯ç”¨çš„ç­”æ¡ˆ
3. ä¼˜å…ˆè§£å†³æœ€ä¸¥é‡çš„è´¨é‡ç¼ºé™·

è¿”å›JSONæ ¼å¼ï¼š
{{
    "questions": [
        "å…·ä½“é—®é¢˜1",
        "å…·ä½“é—®é¢˜2",
        "å…·ä½“é—®é¢˜3"
    ]
}}"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                stream=False,
            )

            result = self._parse_json_response(response)
            return result.get(
                "questions", [f"è¯·æä¾›æ›´å¤šå…³äº{dimension.value}çš„è¯¦ç»†ä¿¡æ¯"]
            )

        except Exception as e:
            logger.error(f"ç”Ÿæˆ {dimension.value} æ¾„æ¸…é—®é¢˜å¤±è´¥: {e}")
            return [f"è¯·è¯¦ç»†æè¿°{dimension.value}ç›¸å…³çš„å…·ä½“è¦æ±‚"]

    def _calculate_priority(
        self, dimension: RequirementDimension, quality: DimensionQuality
    ) -> int:
        """è®¡ç®—æ¾„æ¸…ä¼˜å…ˆçº§"""

        # åŸºç¡€ä¼˜å…ˆçº§
        base_priority = {
            RequirementDimension.FUNCTIONAL: 5,
            RequirementDimension.USER_ROLES: 4,
            RequirementDimension.ACCEPTANCE_CRITERIA: 4,
            RequirementDimension.NON_FUNCTIONAL: 3,
            RequirementDimension.BUSINESS_RULES: 3,
            RequirementDimension.CONSTRAINTS: 2,
            RequirementDimension.DATA_REQUIREMENTS: 2,
            RequirementDimension.INTEGRATION: 1,
        }.get(dimension, 1)

        # è´¨é‡è°ƒæ•´ï¼ˆè´¨é‡è¶Šä½ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        quality_adjustment = max(1, int((1 - quality.overall_score) * 3))

        return min(5, base_priority + quality_adjustment)

    def _estimate_clarification_effort(self, quality: DimensionQuality) -> str:
        """ä¼°ç®—æ¾„æ¸…éš¾åº¦"""
        avg_score = (quality.completeness + quality.clarity + quality.specificity) / 3

        if avg_score < 0.3:
            return "é«˜éš¾åº¦"
        elif avg_score < 0.6:
            return "ä¸­éš¾åº¦"
        else:
            return "ä½éš¾åº¦"

    async def should_continue_clarification(
        self,
        current_quality_assessment: Dict[RequirementDimension, DimensionQuality],
        clarification_count: int = 0,
    ) -> Tuple[bool, str]:
        """
        è´¨é‡ä¸ºæœ¬ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ¾„æ¸…
        """
        # è®¡ç®—æ•´ä½“è´¨é‡
        overall_quality = self._calculate_overall_quality(current_quality_assessment)

        # é˜²æ­¢æ— é™å¾ªç¯
        if clarification_count >= self.max_clarification_attempts:
            return (
                False,
                f"å·²è¾¾åˆ°æœ€å¤§æ¾„æ¸…æ¬¡æ•°({self.max_clarification_attempts})ï¼Œå½“å‰è´¨é‡: {overall_quality:.2f}",
            )

        # æ•´ä½“è´¨é‡è¾¾æ ‡æ£€æŸ¥
        if overall_quality >= self.quality_threshold:
            return (
                False,
                f"âœ… éœ€æ±‚è´¨é‡è¾¾æ ‡ï¼æ•´ä½“è¯„åˆ†: {overall_quality:.2f}/{self.quality_threshold}",
            )

        # æ£€æŸ¥å…³é”®ç»´åº¦è´¨é‡
        critical_dimensions = [
            RequirementDimension.FUNCTIONAL,
            RequirementDimension.USER_ROLES,
            RequirementDimension.ACCEPTANCE_CRITERIA,
        ]

        critical_quality_issues = []
        for dimension in critical_dimensions:
            quality = current_quality_assessment.get(dimension)
            if quality and quality.overall_score < self.dimension_threshold:
                critical_quality_issues.append(
                    f"{dimension.value}: {quality.overall_score:.2f}"
                )

        if critical_quality_issues:
            return (
                True,
                f"ğŸ” å…³é”®ç»´åº¦è´¨é‡ä¸è¾¾æ ‡ï¼Œéœ€è¦ç»§ç»­æ¾„æ¸…: {', '.join(critical_quality_issues)}",
            )

        # æ£€æŸ¥æ˜¯å¦æœ‰ç»´åº¦è´¨é‡è¿‡ä½
        low_quality_dimensions = []
        for dimension, quality in current_quality_assessment.items():
            if quality.overall_score < 0.5:  # è¿‡ä½è´¨é‡é˜ˆå€¼
                low_quality_dimensions.append(
                    f"{dimension.value}: {quality.overall_score:.2f}"
                )

        if low_quality_dimensions:
            return (
                True,
                f"âš ï¸ å‘ç°è´¨é‡è¿‡ä½çš„ç»´åº¦ï¼Œéœ€è¦æ¾„æ¸…: {', '.join(low_quality_dimensions)}",
            )

        return False, f"âœ… éœ€æ±‚è´¨é‡åŸºæœ¬è¾¾æ ‡ï¼Œæ•´ä½“è¯„åˆ†: {overall_quality:.2f}"

    def generate_quality_report(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> str:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š"""

        overall_quality = self._calculate_overall_quality(quality_assessment)

        report = f"""
# éœ€æ±‚è´¨é‡è¯„ä¼°æŠ¥å‘Š

## ğŸ“Š æ•´ä½“è´¨é‡è¯„åˆ†: {overall_quality:.2f}/1.0

## ğŸ“‹ å„ç»´åº¦è¯¦ç»†è¯„ä¼°:

"""

        for dimension, quality in quality_assessment.items():
            status = "âœ…" if quality.overall_score >= self.dimension_threshold else "âš ï¸"
            report += f"""
### {status} {dimension.value} (è¯„åˆ†: {quality.overall_score:.2f})
- **å®Œæ•´æ€§**: {quality.completeness:.2f}
- **æ¸…æ™°åº¦**: {quality.clarity:.2f}
- **å…·ä½“æ€§**: {quality.specificity:.2f}
- **å¯è¡Œæ€§**: {quality.feasibility:.2f}

**ç¼ºå¤±æ–¹é¢**: {', '.join(quality.missing_aspects) if quality.missing_aspects else 'æ— '}
**æ”¹è¿›å»ºè®®**: {', '.join(quality.improvement_suggestions) if quality.improvement_suggestions else 'æ— '}
"""

        # æ·»åŠ æ€»ä½“å»ºè®®
        if overall_quality >= self.quality_threshold:
            report += "\n## ğŸ‰ æ€»ä½“å»ºè®®: éœ€æ±‚è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚"
        else:
            report += f"\n## ğŸ” æ€»ä½“å»ºè®®: éœ€æ±‚è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨è¯„åˆ†ä½äº{self.dimension_threshold}çš„ç»´åº¦ã€‚"

        return report

    def _build_quality_analysis_prompt(
        self, requirement_text: str, clarification_history: List[Dict] = None
    ) -> str:
        """æ„å»ºè´¨é‡åˆ†ææç¤ºè¯"""
        return f"""ä½œä¸ºéœ€æ±‚è´¨é‡åˆ†æä¸“å®¶ï¼Œè¯·å…¨é¢è¯„ä¼°ä»¥ä¸‹éœ€æ±‚çš„è´¨é‡çŠ¶å†µï¼š

éœ€æ±‚æè¿°: "{requirement_text}"

æ¾„æ¸…å†å²: {clarification_history if clarification_history else "æ— "}

è¯·ä»è½¯ä»¶å·¥ç¨‹éœ€æ±‚åˆ†æçš„è§’åº¦ï¼Œç³»ç»Ÿæ€§è¯„ä¼°éœ€æ±‚åœ¨å„ä¸ªç»´åº¦çš„è´¨é‡æ°´å¹³ã€‚"""

    def _parse_json_response(self, response: str) -> Dict:
        """è§£æJSONå“åº”"""
        import json
        import re

        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except:
            try:
                # æå–JSONéƒ¨åˆ†
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    return {}
            except:
                logger.error(f"JSONè§£æå¤±è´¥: {response}")
                return {}

    def _calculate_goal_oriented_score(
        self, analysis_result: dict, clarification_history: List[dict]
    ) -> float:
        """
        è®¡ç®—ç›®æ ‡å¯¼å‘è¯„åˆ† - ä¿®æ­£ç‰ˆ
        è¯„ä¼°æ˜¯å¦æœ‰æ•ˆè¾¾æˆæœ€ç»ˆç›®æ ‡ï¼šç”Ÿæˆé«˜è´¨é‡éœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦

        ä¸å†ä»¥è½®æ¬¡æ•°é‡ä¸ºæŒ‡æ ‡ï¼Œè€Œæ˜¯è¯„ä¼°ç›®æ ‡è¾¾æˆçš„æœ‰æ•ˆæ€§
        """
        goal_achievement_score = 0.0

        # 1. æœ€ç»ˆç›®æ ‡å®Œæˆåº¦è¯„ä¼° (40%)
        final_quality = analysis_result.get("final_quality_score", 0.0)
        goal_completion = min(final_quality / 0.8, 1.0)  # ç›®æ ‡æ˜¯è¾¾åˆ°0.8è´¨é‡
        goal_achievement_score += goal_completion * 0.4

        # 2. ç›®æ ‡è¾¾æˆæ•ˆç‡è¯„ä¼° (30%)
        # è¯„ä¼°è¾¾æˆç›®æ ‡çš„æ•ˆç‡ï¼Œè€Œéå•çº¯çš„è½®æ¬¡å¤šå°‘
        total_rounds = len(clarification_history)
        if total_rounds > 0:
            quality_improvement_per_round = final_quality / total_rounds
            efficiency_score = min(quality_improvement_per_round * 5, 1.0)  # å½’ä¸€åŒ–
            goal_achievement_score += efficiency_score * 0.3

        # 3. ç›®æ ‡ä¸€è‡´æ€§è¯„ä¼° (20%)
        # è¯„ä¼°æ•´ä¸ªè¿‡ç¨‹æ˜¯å¦å§‹ç»ˆå›´ç»•æœ€ç»ˆç›®æ ‡è¿›è¡Œ
        consistency_score = self._evaluate_goal_consistency(clarification_history)
        goal_achievement_score += consistency_score * 0.2

        # 4. ç›®æ ‡ä»·å€¼å®ç°è¯„ä¼° (10%)
        # è¯„ä¼°æ˜¯å¦å®ç°äº†ç”¨æˆ·çš„çœŸå®ç›®æ ‡ä»·å€¼
        value_realization = self._evaluate_value_realization(analysis_result)
        goal_achievement_score += value_realization * 0.1

        return min(goal_achievement_score, 1.0)

    def _evaluate_goal_consistency(self, clarification_history: List[dict]) -> float:
        """è¯„ä¼°ç›®æ ‡ä¸€è‡´æ€§ï¼šæ•´ä¸ªè¿‡ç¨‹æ˜¯å¦å§‹ç»ˆå›´ç»•æœ€ç»ˆç›®æ ‡"""
        if not clarification_history:
            return 0.0

        consistency_indicators = []

        for record in clarification_history:
            questions = record.get("questions", [])
            # è¯„ä¼°é—®é¢˜æ˜¯å¦éƒ½æŒ‡å‘éœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦çš„å…³é”®è¦ç´ 
            relevant_questions = 0
            total_questions = len(questions)

            for question in questions:
                question_text = question.get("question", "").lower()
                # æ£€æŸ¥é—®é¢˜æ˜¯å¦é’ˆå¯¹éœ€æ±‚æ–‡æ¡£çš„æ ¸å¿ƒè¦ç´ 
                core_elements = [
                    "åŠŸèƒ½éœ€æ±‚",
                    "éåŠŸèƒ½éœ€æ±‚",
                    "ç”¨æˆ·è§’è‰²",
                    "ä¸šåŠ¡è§„åˆ™",
                    "çº¦æŸæ¡ä»¶",
                    "éªŒæ”¶æ ‡å‡†",
                    "æŠ€æœ¯æ–¹æ¡ˆ",
                    "å®ç°æ–¹å¼",
                ]
                if any(element in question_text for element in core_elements):
                    relevant_questions += 1

            if total_questions > 0:
                consistency_indicators.append(relevant_questions / total_questions)

        return (
            sum(consistency_indicators) / len(consistency_indicators)
            if consistency_indicators
            else 0.0
        )

    def _evaluate_value_realization(self, analysis_result: dict) -> float:
        """è¯„ä¼°ä»·å€¼å®ç°ï¼šæ˜¯å¦å®ç°äº†ç”¨æˆ·çš„çœŸå®ç›®æ ‡ä»·å€¼"""
        value_score = 0.0

        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº†éœ€æ±‚æ–‡æ¡£çš„å…³é”®éƒ¨åˆ†
        key_components = [
            "functional_requirements",  # åŠŸèƒ½éœ€æ±‚
            "non_functional_requirements",  # éåŠŸèƒ½éœ€æ±‚
            "user_stories",  # ç”¨æˆ·æ•…äº‹
            "acceptance_criteria",  # éªŒæ”¶æ ‡å‡†
            "technical_constraints",  # æŠ€æœ¯çº¦æŸ
            "business_rules",  # ä¸šåŠ¡è§„åˆ™
        ]

        present_components = 0
        for component in key_components:
            if component in analysis_result:
                present_components += 1

        value_score = present_components / len(key_components)

        return value_score

    async def generate_clarification_questions(
        self, clarification_goals: List[ClarificationGoal]
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºæ¾„æ¸…ç›®æ ‡ç”Ÿæˆå…·ä½“çš„æ¾„æ¸…é—®é¢˜
        """

        questions = []

        for goal in clarification_goals[:3]:  # é™åˆ¶æœ€å¤š3ä¸ªç›®æ ‡ï¼Œé¿å…è¿‡å¤šé—®é¢˜
            for question_text in goal.key_questions[:2]:  # æ¯ä¸ªç›®æ ‡æœ€å¤š2ä¸ªé—®é¢˜
                question = {
                    "question": question_text,
                    "dimension": goal.dimension.value,
                    "priority": goal.priority,
                    "estimated_effort": goal.estimated_effort,
                    "target_quality": goal.target_quality,
                }
                questions.append(question)

        return questions
