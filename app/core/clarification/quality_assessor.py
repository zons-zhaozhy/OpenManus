"""
éœ€æ±‚è´¨é‡è¯„ä¼°å™¨
è´Ÿè´£è¯„ä¼°éœ€æ±‚åœ¨å„ä¸ªç»´åº¦çš„è´¨é‡ï¼Œæ”¯æŒè´¨é‡é©±åŠ¨çš„æ¾„æ¸…ç­–ç•¥
"""

import asyncio
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List

from loguru import logger

from app.config import REQUIREMENT_QUALITY_CONFIG
from app.llm import LLM


class RequirementDimension(Enum):
    """éœ€æ±‚ç»´åº¦æšä¸¾"""

    FUNCTIONAL = "åŠŸèƒ½éœ€æ±‚"
    NON_FUNCTIONAL = "éåŠŸèƒ½éœ€æ±‚"
    USER_ROLES = "ç”¨æˆ·è§’è‰²"
    BUSINESS_RULES = "ä¸šåŠ¡è§„åˆ™"
    CONSTRAINTS = "çº¦æŸæ¡ä»¶"
    ACCEPTANCE_CRITERIA = "éªŒæ”¶æ ‡å‡†"
    INTEGRATION = "é›†æˆéœ€æ±‚"
    DATA_REQUIREMENTS = "æ•°æ®éœ€æ±‚"


@dataclass
class DimensionQuality:
    """ç»´åº¦è´¨é‡è¯„ä¼°"""

    dimension: RequirementDimension
    completeness: float  # å®Œæ•´æ€§ 0-1
    clarity: float  # æ¸…æ™°åº¦ 0-1
    specificity: float  # å…·ä½“æ€§ 0-1
    feasibility: float  # å¯è¡Œæ€§ 0-1
    overall_score: float  # ç»¼åˆè¯„åˆ† 0-1
    missing_aspects: List[str]  # ç¼ºå¤±æ–¹é¢
    improvement_suggestions: List[str]  # æ”¹è¿›å»ºè®®


class QualityAssessor:
    """éœ€æ±‚è´¨é‡è¯„ä¼°å™¨"""

    def __init__(self):
        self.llm = LLM()
        self.config = REQUIREMENT_QUALITY_CONFIG

        # è´¨é‡é˜ˆå€¼
        self.quality_threshold = self.config["quality_thresholds"]["overall_threshold"]
        self.dimension_threshold = self.config["quality_thresholds"][
            "dimension_threshold"
        ]
        self.excellent_threshold = self.config["quality_thresholds"][
            "excellent_threshold"
        ]

        # ç»´åº¦æƒé‡
        self.dimension_weights = self.config["dimension_weights"]

        # è¯„åˆ†ä¸¥æ ¼åº¦é…ç½®
        self.scoring_config = self.config["scoring_strictness"]

    async def analyze_requirement_quality(
        self, requirement_text: str, clarification_history: List[Dict] = None
    ) -> Dict[RequirementDimension, DimensionQuality]:
        """
        åˆ†æéœ€æ±‚è´¨é‡ - å¤šç»´åº¦å¹¶è¡Œè¯„ä¼°
        """
        logger.info("ğŸ¯ å¼€å§‹å¤šç»´åº¦éœ€æ±‚è´¨é‡åˆ†æ...")

        # å¹¶è¡Œåˆ†ææ‰€æœ‰ç»´åº¦
        dimension_tasks = []
        for dimension in RequirementDimension:
            task = self._analyze_single_dimension(
                dimension, requirement_text, clarification_history
            )
            dimension_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
        dimension_results = await asyncio.gather(*dimension_tasks)

        # æ„å»ºè´¨é‡è¯„ä¼°ç»“æœ
        quality_assessment = {}
        for i, dimension in enumerate(RequirementDimension):
            quality_assessment[dimension] = dimension_results[i]

        # è®°å½•è´¨é‡è¯„ä¼°æ—¥å¿—
        overall_quality = self._calculate_overall_quality(quality_assessment)
        logger.info(f"ğŸ“Š éœ€æ±‚æ•´ä½“è´¨é‡è¯„åˆ†: {overall_quality:.2f}")

        return quality_assessment

    async def _analyze_single_dimension(
        self,
        dimension: RequirementDimension,
        requirement_text: str,
        clarification_history: List[Dict] = None,
    ) -> DimensionQuality:
        """åˆ†æå•ä¸ªéœ€æ±‚ç»´åº¦çš„è´¨é‡"""

        prompt = f"""ä½œä¸ºéœ€æ±‚åˆ†æä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹éœ€æ±‚åœ¨ã€{dimension.value}ã€‘ç»´åº¦çš„è´¨é‡ï¼š

éœ€æ±‚æ–‡æœ¬ï¼š"{requirement_text}"

æ¾„æ¸…å†å²ï¼š{clarification_history if clarification_history else "æ— "}

è¯·ä»ä»¥ä¸‹è§’åº¦è¯„ä¼°ï¼ˆåŠ¡å®æ ‡å‡†ï¼Œä¸è¦è¿‡äºè‹›åˆ»ï¼‰ï¼š

1. å®Œæ•´æ€§ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: æ ¸å¿ƒä¿¡æ¯åŸºæœ¬é½å…¨
   - 0.5-0.7: ä¸»è¦å†…å®¹æ˜ç¡®ï¼Œéƒ¨åˆ†ç»†èŠ‚å¾…è¡¥å……
   - 0.3-0.5: åŸºæœ¬æ¦‚å¿µæ¸…æ¥šï¼Œç¼ºå°‘é‡è¦ç»†èŠ‚
   - 0.0-0.3: ä¿¡æ¯ä¸¥é‡ä¸è¶³

2. æ¸…æ™°åº¦ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: è¡¨è¿°æ¸…æ™°æ˜“æ‡‚
   - 0.5-0.7: å¤§éƒ¨åˆ†å†…å®¹æ¸…æ¥š
   - 0.3-0.5: åŸºæœ¬èƒ½ç†è§£ï¼Œéœ€è¦æ¾„æ¸…
   - 0.0-0.3: è¡¨è¿°æ¨¡ç³Šéš¾æ‡‚

3. å…·ä½“æ€§ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: å…·ä½“è¯¦ç»†ï¼Œå¯æ‰§è¡Œ
   - 0.5-0.7: æœ‰ä¸€å®šç»†èŠ‚
   - 0.3-0.5: è¾ƒä¸ºæŠ½è±¡ï¼Œéœ€è¦å…·ä½“åŒ–
   - 0.0-0.3: è¿‡äºç¬¼ç»Ÿ

4. å¯è¡Œæ€§ï¼ˆ0-1ï¼‰ï¼š
   - 0.7-1.0: æŠ€æœ¯å¯è¡Œï¼Œèµ„æºåˆç†
   - 0.5-0.7: åŸºæœ¬å¯è¡Œï¼Œæœ‰æŒ‘æˆ˜
   - 0.3-0.5: å¯è¡Œæ€§å­˜ç–‘
   - 0.0-0.3: ä¸å¯è¡Œæˆ–é£é™©æé«˜

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "completeness": 0.0-1.0,
    "clarity": 0.0-1.0,
    "specificity": 0.0-1.0,
    "feasibility": 0.0-1.0,
    "missing_aspects": ["ç¼ºå¤±æ–¹é¢1", "ç¼ºå¤±æ–¹é¢2"],
    "improvement_suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""

        try:
            response = await self.llm.achat(prompt)
            analysis = self._parse_json_response(response)

            # è®¡ç®—ç»¼åˆè¯„åˆ†
            overall_score = (
                analysis["completeness"] * 0.3
                + analysis["clarity"] * 0.25
                + analysis["specificity"] * 0.25
                + analysis["feasibility"] * 0.2
            )

            return DimensionQuality(
                dimension=dimension,
                completeness=analysis["completeness"],
                clarity=analysis["clarity"],
                specificity=analysis["specificity"],
                feasibility=analysis["feasibility"],
                overall_score=overall_score,
                missing_aspects=analysis.get("missing_aspects", []),
                improvement_suggestions=analysis.get("improvement_suggestions", []),
            )

        except Exception as e:
            logger.error(f"ç»´åº¦ {dimension.value} è´¨é‡åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„ä½è´¨é‡è¯„ä¼°
            return DimensionQuality(
                dimension=dimension,
                completeness=0.3,
                clarity=0.3,
                specificity=0.3,
                feasibility=0.5,
                overall_score=0.3,
                missing_aspects=[f"{dimension.value}ä¿¡æ¯ä¸è¶³"],
                improvement_suggestions=[f"è¯·è¡¥å……{dimension.value}ç›¸å…³ä¿¡æ¯"],
            )

    def _calculate_overall_quality(
        self, quality_assessment: Dict[RequirementDimension, DimensionQuality]
    ) -> float:
        """
        è®¡ç®—éœ€æ±‚æ•´ä½“è´¨é‡åˆ†æ•°
        """
        if not quality_assessment:
            return 0.0

        # åŠ æƒè®¡ç®—æ•´ä½“è´¨é‡
        total_weighted_score = 0.0
        total_weight = 0.0

        for dimension, quality in quality_assessment.items():
            weight = self.dimension_weights.get(dimension.value, 1.0)
            total_weighted_score += quality.overall_score * weight
            total_weight += weight

        # é¿å…é™¤é›¶é”™è¯¯
        if total_weight == 0:
            return 0.0

        overall_score = total_weighted_score / total_weight

        logger.info(f"è´¨é‡è¯„ä¼°è¯¦æƒ…:")
        for dimension, quality in quality_assessment.items():
            logger.info(f"  {dimension.value}: {quality.overall_score:.2f}")

        return overall_score

    def _parse_json_response(self, response: str) -> Dict:
        """è§£æLLMçš„JSONå“åº”"""
        try:
            import json

            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except json.JSONDecodeError:
            try:
                # å°è¯•æå–JSONéƒ¨åˆ†
                start = response.find("{")
                end = response.rfind("}") + 1
                if start != -1 and end != 0:
                    json_str = response[start:end]
                    return json.loads(json_str)
                else:
                    logger.warning(f"æ— æ³•æå–JSON: {response}")
                    return self._get_default_response()
            except Exception as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return self._get_default_response()
        except Exception as e:
            logger.error(f"è§£æå“åº”å¤±è´¥: {e}")
            return self._get_default_response()

    def _get_default_response(self) -> Dict:
        """è¿”å›é»˜è®¤å“åº”"""
        return {
            "completeness": 0.5,
            "clarity": 0.5,
            "specificity": 0.5,
            "feasibility": 0.7,
            "missing_aspects": ["éœ€è¦æ›´å¤šä¿¡æ¯"],
            "improvement_suggestions": ["è¯·æä¾›æ›´è¯¦ç»†çš„æè¿°"],
        }

    def should_continue_clarification(
        self,
        quality_assessment: Dict[RequirementDimension, DimensionQuality],
        current_round: int,
    ) -> tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ¾„æ¸…

        Returns:
            (should_continue: bool, reason: str)
        """
        overall_quality = self._calculate_overall_quality(quality_assessment)

        # è·å–é…ç½®å‚æ•°
        max_rounds = self.config["clarification_rounds"]["max_rounds"]
        min_rounds = self.config["clarification_rounds"]["min_rounds"]
        early_stop_threshold = self.config["clarification_rounds"][
            "early_stop_threshold"
        ]

        # è½®æ¬¡é™åˆ¶æ£€æŸ¥
        if current_round >= max_rounds:
            return False, f"å·²è¾¾åˆ°æœ€å¤§æ¾„æ¸…è½®æ¬¡ ({max_rounds})"

        # è´¨é‡æ»¡è¶³æ£€æŸ¥
        if overall_quality >= early_stop_threshold and current_round >= min_rounds:
            return (
                False,
                f"è´¨é‡å·²è¾¾æ ‡ ({overall_quality:.2f} >= {early_stop_threshold})",
            )

        # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®ç»´åº¦è´¨é‡è¿‡ä½
        critical_issues = []
        for dimension, quality in quality_assessment.items():
            if quality.overall_score < self.dimension_threshold:
                critical_issues.append(
                    f"{dimension.value}({quality.overall_score:.2f})"
                )

        if critical_issues:
            return True, f"å…³é”®ç»´åº¦è´¨é‡ä¸è¶³: {', '.join(critical_issues)}"

        if overall_quality < self.quality_threshold:
            return (
                True,
                f"æ•´ä½“è´¨é‡ä¸è¶³ ({overall_quality:.2f} < {self.quality_threshold})",
            )

        return False, f"è´¨é‡åŸºæœ¬æ»¡è¶³è¦æ±‚ ({overall_quality:.2f})"
